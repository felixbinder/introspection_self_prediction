{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the ceiling performance for a model on the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR\n",
    "from evals.utils import run_command\n",
    "from evals.analysis.loading_data import get_hydra_config\n",
    "from evals.analysis.loading_data import load_single_df_from_exp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which models, and which tasks?\n",
    "Using the format from `scripts/sweep_full_study.py`.\n",
    "\n",
    "`TASKS` is a string of a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"may20_thrifty_sweep\"\n",
    "MODELS = [\"claude-3-sonnet\",\"gpt-3.5-turbo\",\"gpt-4\",\"gemini-1.0-pro-002\"]\n",
    "TASKS = '{\"number_triplets\": [\"identity\", \"is_even\", \"last_character\", \"first_character\"], \"wikipedia\": [\"identity\", \"syllable_count\", \"first_character\", \"last_character\"], \"writing_stories\": [\"identity\", \"first_word\", \"writing_stories/main_character_name\"], \"personal_preferences\": [\"identity\", \"syllable_count\", \"first_character\", \"last_character\"], }' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other hyperparameters\n",
    "N_PER_TASK = 10\n",
    "SEED = 42\n",
    "SAMPLES_PER_INPUT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = eval(TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ceiling calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm(MODELS):\n",
    "    for task in TASKS.keys():\n",
    "        command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_object_level.py study_name={'nondeterminism_ceiling/'+STUDY_NAME} task={task} language_model={model} task.set=val n_samples={SAMPLES_PER_INPUT} task.num={N_PER_TASK}\"\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Running {model} on {task}: {command}\")\n",
    "        run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the response properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = EXP_DIR / \"nondeterminism_ceiling\" / STUDY_NAME\n",
    "subfolders = [results_folder / f for f in next(os.walk(results_folder))[1]]\n",
    "print(f\"Got {len(subfolders)} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in tqdm(subfolders):\n",
    "        # load config\n",
    "        try:\n",
    "                cfg = get_hydra_config(folder)\n",
    "        except ValueError:\n",
    "                print(f\"Skipping {folder}\")\n",
    "                continue\n",
    "        task = cfg.task.name\n",
    "        response_properties = TASKS[task]\n",
    "        for response_property in response_properties:\n",
    "                command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_property_extraction.py dir={folder} response_property={response_property}\"\n",
    "                print(f\"üõ∏ Extracting {response_property} on {model} on {task}: {command}\")\n",
    "                try:\n",
    "                        run_command(command)\n",
    "                except Exception as e:\n",
    "                        print(f\"Error: {e}\\nwhile running {command}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_match(df_subset, response_property='identity'):\n",
    "    # assert len(df_subset) == N_SAMPLES, f\"Expected {N_SAMPLES} samples, got {len(df_subset)}\"\n",
    "    assert df_subset['string'].nunique() == 1, \"Expected all samples to be from the same string\"\n",
    "    responses = df_subset[response_property].values\n",
    "    shuffled_responses = np.random.permutation(responses)\n",
    "    return np.mean(responses == shuffled_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_95_CI(samples):\n",
    "    means = []\n",
    "    for _ in range(BOOTSTRAP_N):\n",
    "        sample = np.random.choice(samples, len(samples), replace=True)\n",
    "        means.append(np.mean(sample))\n",
    "    return np.percentile(means, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ceiling(folder, response_property):\n",
    "    # load df\n",
    "    df = load_single_df_from_exp_path(folder, exclude_noncompliant=False) # TODO Should this be true? That might increase the ceiling.\n",
    "    samples_across_strings = []\n",
    "    means_across_strings = []\n",
    "\n",
    "    for string in tqdm(df.string.unique()):\n",
    "        samples_across_iters = [compute_pairwise_match(df[df.string == string], response_property) for _ in range(BOOTSTRAP_N)]\n",
    "        samples_across_strings.append(samples_across_iters)\n",
    "        means_across_strings.append(np.mean(samples_across_iters))\n",
    "    \n",
    "    all_samples = np.concatenate(samples_across_strings)\n",
    "    # return mean and 95%CI of mean\n",
    "    return np.mean(means_across_strings), bootstrap_95_CI(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_results = {}\n",
    "\n",
    "for folder in tqdm(subfolders):\n",
    "    try:\n",
    "        cfg = get_hydra_config(folder)\n",
    "    except ValueError:\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    task = cfg.task.name\n",
    "    model = cfg.language_model.model\n",
    "    response_properties = TASKS[task]\n",
    "    for response_property in response_properties:\n",
    "        mean, ci = compute_ceiling(folder, response_property)\n",
    "        ceiling_results[(model, task, response_property)] = (mean, ci)\n",
    "\n",
    "ceiling_results_df = pd.DataFrame(ceiling_results).T\n",
    "ceiling_results_df.columns = ['mean', 'ci']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated mean by model\n",
    "display(ceiling_results_df['mean'].groupby(level=0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results\n",
    "as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceiling_results_df.to_csv(EXP_DIR / \"nondeterminism_ceiling\" / f\"{STUDY_NAME}_ceiling_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
