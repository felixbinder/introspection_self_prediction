{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How deterministic are models?\n",
    "It looks like models at temperature = 0 still have a lot of randomness in them. This notebook will explore how deterministic models are at temperature = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR\n",
    "from evals.utils import run_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo-0125\"\n",
    "# MODEL = \"claude-3-sonnet\"\n",
    "STUDY_NAME = \"how_deterministic_are_models\"\n",
    "# TASK = \"daily_dialog\"\n",
    "# TASK = \"number_triplets\"\n",
    "TASK = \"writing_stories\"\n",
    "N_SAMPLES = 100\n",
    "N_STRINGS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a bunch of samples of the same string from the same model at temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_object_level.py study_name={STUDY_NAME} task={TASK} language_model={MODEL} task.set=val n_samples={N_SAMPLES} task.num={N_STRINGS}\"\n",
    "folder_name = run_command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.loading_data import load_single_df_from_exp_path\n",
    "from evals.locations import EXP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_single_df_from_exp_path(folder_name, exclude_noncompliant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique answers per input?\n",
    "df.groupby(\"string\").response.nunique().hist()\n",
    "plt.title(f\"Number of unique responses on the same input (out of {N_SAMPLES} samples) on {TASK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compute upper bound: 100 samples from object-levelâ€”what is the chance that two samples match? -> [ ] try on non-determinism notebook\n",
    "\n",
    "We want to know what the chance is that two given samples match given the level of non-determinacy. We compute this by taking the single 100 sample, permuting it, and seeing how many pairwise matches we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_match(df_subset):\n",
    "    # assert len(df_subset) == N_SAMPLES, f\"Expected {N_SAMPLES} samples, got {len(df_subset)}\"\n",
    "    assert df_subset['string'].nunique() == 1, \"Expected all samples to be from the same string\"\n",
    "    responses = df_subset['response'].values\n",
    "    shuffled_responses = np.random.permutation(responses)\n",
    "    return np.mean(responses == shuffled_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_across_strings = []\n",
    "means_across_strings = []\n",
    "\n",
    "for string in tqdm(df.string.unique()):\n",
    "    samples_across_iters = [compute_pairwise_match(df[df.string == string]) for _ in range(BOOTSTRAP_N)]\n",
    "    samples_across_strings.append(samples_across_iters)\n",
    "    means_across_strings.append(np.mean(samples_across_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(means_across_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
