{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-prediction analysis\n",
    "2024-09-06: this notebook reproduces the analysis of cross-prediction evidence over `paper_figures.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY = \"23_jul_fixed_tasks_medium_cross\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pairs = [\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\",\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\",\n",
    "    ),  # A_fton_A predicting A_fton_A\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\",\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P\",\n",
    "    ),  # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # and vice versa\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "    ),\n",
    "    (  # B_fton_B predicting B_fton_B\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF\",\n",
    "    ),  # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    # (  # DEBUG\n",
    "    #     \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF\",\n",
    "    #     \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "    # ),\n",
    "    # (  # DEBUG\n",
    "    #     \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "    #     \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF\",\n",
    "    # ),\n",
    "]\n",
    "\n",
    "tasks_and_response_properties = {  # 23_jul_fixed_tasks_medium_cross\n",
    "    # test set\n",
    "    # \"writing_stories_pick_name\": [\"writing_stories/main_character_name\"],\n",
    "    # \"wikipedia_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"wealth_seeking\": [\"matches_wealth_seeking\"],\n",
    "    # \"power_seeking\": [\"matches_power_seeking\"],\n",
    "    # \"arc_challenge_non_cot\": [\"identity\", \"is_either_a_or_c\", \"is_either_b_or_d\"],\n",
    "    # \"countries_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"colors_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"numbers\": [\n",
    "    #     \"is_even_direct\",\n",
    "    #     # \"is_even\" # broken, but we only need is_even_direct\n",
    "    # ],\n",
    "    # val set\n",
    "    \"survival_instinct\": [\"matches_survival_instinct\"],\n",
    "    \"myopic_reward\": [\"matches_myopic_reward\"],\n",
    "    \"animals_long\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "    \"mmlu_non_cot\": [\"is_either_a_or_c\", \"is_either_b_or_d\"],\n",
    "    \"english_words_long\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "    \"stories_sentences\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from p_tqdm import p_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import (\n",
    "    merge_object_and_meta_dfs,\n",
    "    create_df_from_configs,\n",
    "    fill_df_with_function,\n",
    "    get_pretty_name,\n",
    "    filter_configs_by_conditions,\n",
    "    pretty_print_config,\n",
    "    get_pretty_name_w_labels,\n",
    "    merge_object_and_meta_dfs_and_run_property_extraction,\n",
    ")\n",
    "from evals.analysis.loading_data import (\n",
    "    load_dfs_with_filter,\n",
    "    load_base_df_from_config,\n",
    "    get_hydra_config,\n",
    "    load_single_df,\n",
    "    load_single_df_from_exp_path,\n",
    "    get_data_path,\n",
    "    get_folders_matching_config_key,\n",
    ")\n",
    "from evals.load.lazy_object_level_llm_extraction import lazy_add_response_property_to_object_level\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.analysis.analysis_helpers import bootstrap_ci, compute_standard_error_ci, compute_binary_ci\n",
    "from evals.locations import EXP_DIR\n",
    "\n",
    "print(f\"EXP_DIR: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "- for each model pair, load each response property individually\n",
    "    - based on the --tasks property\n",
    "- compute exclusions & merge\n",
    "- compute accuracy\n",
    "- compute mode baseline\n",
    "- save to big table with model pair, property and accuracy\n",
    "- plot the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs_for_model_pair_and_property(object_model, meta_model, task, response_property): # slowâ€”takes ~30s\n",
    "    # load object df\n",
    "    conditions = {('language_model', 'model'): [object_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [None]}\n",
    "    exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "    assert len(exp_paths) == 1, f\"Expected 1 experiment path for object level, got {len(exp_paths)}\"\n",
    "    object_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "    # load meta df\n",
    "    conditions = {('language_model', 'model'): [meta_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [response_property]}\n",
    "    exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "    assert len(exp_paths) == 1, f\"Expected 1 experiment path for meta level, got {len(exp_paths)}\"\n",
    "    meta_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "    return object_df, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(object_df, meta_df):\n",
    "    # Assert that strings are unique in both dataframes\n",
    "    assert object_df['string'].nunique() == len(object_df), \"Strings in object_df are not unique\"\n",
    "    assert meta_df['string'].nunique() == len(meta_df), \"Strings in meta_df are not unique\"\n",
    "    # Rename columns in object_df\n",
    "    object_df = object_df.add_prefix('obj_')\n",
    "    \n",
    "    # Rename columns in meta_df\n",
    "    meta_df = meta_df.add_prefix('meta_')\n",
    "    \n",
    "    # Merge the dataframes on the string column\n",
    "    merged_df = pd.merge(object_df, meta_df, left_on='obj_string', right_on='meta_string', how='inner')\n",
    "    \n",
    "    # did we loose all rows?\n",
    "    assert len(merged_df) > 0, \"No rows left after merging\"\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_baseline_for_model_pair_and_property(object_model, meta_model, task, response_property):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy and baseline for a given model pair and response property.\n",
    "\n",
    "    Args:\n",
    "    object_model (str): The name of the object-level model.\n",
    "    meta_model (str): The name of the meta-level model.\n",
    "    task (str): The name of the task.\n",
    "    response_property (str): The specific response property to analyze.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - accuracy (float): The accuracy of the model pair.\n",
    "        - sem (float): The standard error of the mean for the accuracy.\n",
    "        - mode_acc (float): The accuracy of the mode baseline.\n",
    "        - mode_sem (float): The standard error of the mean for the mode baseline.\n",
    "    \"\"\"\n",
    "    object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, response_property)\n",
    "    merged_df = merge_dfs(object_df, meta_df)\n",
    "    accuracy, sem = compute_accuracy(merged_df, response_property)\n",
    "    mode_acc, mode_sem = mode_baseline_accuracy(object_df, response_property)\n",
    "    return accuracy, sem, mode_acc, mode_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(merged_df, response_property):\n",
    "    \"\"\"Computes the accuracy of the model pair.\n",
    "    Performs exclusions according to the following rules:\n",
    "    - if object level response is non-compliant, exclude the row\n",
    "    - if meta level response is non-compliant, count the prediction as incorrect\n",
    "\n",
    "    CI is the standard error of the mean\n",
    "    \"\"\"\n",
    "    correctnesses = get_correctnesses(merged_df, response_property)\n",
    "    # compute accuracy\n",
    "    acc = correctnesses.mean()\n",
    "    sem = stats.sem(correctnesses)\n",
    "    return acc, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctnesses(merged_df, response_property):\n",
    "    assert \"meta_\" + response_property in merged_df.columns, f\"Response property {response_property} not found in meta_df\"\n",
    "    assert \"obj_\" + response_property in merged_df.columns, f\"Response property {response_property} not found in object_df\"\n",
    "    # cast the response cols to string and ensure thy're lowercase\n",
    "    merged_df['obj_' + response_property] = merged_df['obj_' + response_property].astype(str).str.lower()\n",
    "    merged_df['meta_' + response_property] = merged_df['meta_' + response_property].astype(str).str.lower()\n",
    "    # get correctness\n",
    "    merged_df['correct'] = merged_df['obj_' + response_property] == merged_df['meta_' + response_property]\n",
    "    # Exclusion rules\n",
    "    # if object level response is non-compliant, exclude the row\n",
    "    excluded_mask = merged_df['obj_compliance'] != True\n",
    "    if excluded_mask.any():\n",
    "        merged_df = merged_df[~excluded_mask]\n",
    "    # if meta level response is non-compliant, count the prediction as incorrect\n",
    "    excluded_mask = merged_df['meta_compliance'] != True\n",
    "    if excluded_mask.any():\n",
    "        merged_df.loc[excluded_mask, 'correct'] = False\n",
    "    return merged_df['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_baseline_accuracy(object_df, response_property):\n",
    "    \"\"\"How well would you do if you always predicted the mode of the distribution?\"\"\"\n",
    "    # Create an explicit copy of the DataFrame\n",
    "    df = object_df.copy()\n",
    "    \n",
    "    # exclude non-compliant responses\n",
    "    df = df[df['compliance'] == True]\n",
    "    \n",
    "    # compute mode\n",
    "    mode = df[response_property].mode()[0]\n",
    "    \n",
    "    # Use .loc to set values\n",
    "    df.loc[:, 'correct'] = df[response_property] == mode\n",
    "    \n",
    "    acc = df['correct'].mean()\n",
    "    sem = stats.sem(df['correct'])\n",
    "    return acc, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracies_across_models_pairs(model_pairs, tasks_and_response_properties):\n",
    "    \"\"\"\n",
    "    Calculate accuracies for a list of model pairs and tasks/response properties.\n",
    "\n",
    "    Args:\n",
    "    model_pairs (list of tuples): Each tuple contains two models to be compared. Include the language_model.name field, not the name of the config!\n",
    "    tasks_and_response_properties (dict): according to the structure of the sweep script, eg:\n",
    "    {\"writing_stories_pick_name\": [\"writing_stories/main_character_name\"], \"wikipedia_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"wealth_seeking\": [\"matches_wealth_seeking\"], \"power_seeking\": [\"matches_power_seeking\"], \"arc_challenge_non_cot\": [\"identity\", \"is_either_a_or_c\", \"is_either_b_or_d\"], \"countries_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"colors_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"numbers\": [\"is_even_direct\", \"is_even\"]}\n",
    "\n",
    "    Returns:\n",
    "        Multi-index dataframe with indices:\n",
    "            1. object level model (prediction target)\n",
    "            2. meta level model (predictor)\n",
    "            3. task\n",
    "            4. response property\n",
    "        and columns:\n",
    "            1. accuracy\n",
    "            2. standard error of the mean\n",
    "            3. mode baseline accuracy\n",
    "            4. standard error of the mode baseline\n",
    "    \"\"\"\n",
    "    # Initialize an empty dataframe with a MultiIndex\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (model_pair[0], model_pair[1], task, prop)\n",
    "            for model_pair in model_pairs\n",
    "            for task, props in tasks_and_response_properties.items()\n",
    "            for prop in props\n",
    "        ],\n",
    "        names=[\"object_model\", \"meta_model\", \"task\", \"response_property\"],\n",
    "    )\n",
    "    columns = [\"accuracy\", \"sem\", \"mode_baseline_accuracy\", \"mode_baseline_sem\"]\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    def process_model_pair(args):\n",
    "        object_model, meta_model, task, prop = args\n",
    "        try:\n",
    "            accuracy, sem, mode_acc, mode_sem = get_accuracy_and_baseline_for_model_pair_and_property(\n",
    "                object_model, meta_model, task, prop\n",
    "            )\n",
    "            return (object_model, meta_model, task, prop), (accuracy, sem, mode_acc, mode_sem)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            return (object_model, meta_model, task, prop), (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (model_pair[0], model_pair[1], task, prop)\n",
    "        for model_pair in model_pairs\n",
    "        for task, props in tasks_and_response_properties.items()\n",
    "        for prop in props\n",
    "    ]\n",
    "\n",
    "    # Use p_umap for parallel processing\n",
    "    results = p_umap(process_model_pair, args_list)\n",
    "    # non parallel for debugging\n",
    "    # results = []\n",
    "    # for args in args_list:\n",
    "    #     result = process_model_pair(args)\n",
    "    #     results.append(result)\n",
    "\n",
    "    # Fill the dataframe with results\n",
    "    for idx, (acc, sem, mode_acc, mode_sem) in results:\n",
    "        df.loc[idx] = [acc, sem, mode_acc, mode_sem]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = calculate_accuracies_across_models_pairs(\n",
    "    model_pairs,\n",
    "    tasks_and_response_properties,\n",
    ")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_tqdm import p_map\n",
    "\n",
    "def calculate_overall_accuracies(model_pairs, tasks_and_response_properties):\n",
    "    \"\"\"\n",
    "    Calculate overall accuracies for a list of model pairs across all tasks and response properties combined.\n",
    "\n",
    "    Args:\n",
    "    model_pairs (list of tuples): Each tuple contains two models to be compared.\n",
    "    tasks_and_response_properties (dict): Dictionary of tasks and their response properties.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with overall accuracies, SEMs, and sample sizes for each model pair.\n",
    "    \"\"\"\n",
    "    def process_model_pair(model_pair):\n",
    "        object_model, meta_model = model_pair\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        correctnesses = []\n",
    "\n",
    "        for task, props in tasks_and_response_properties.items():\n",
    "            for prop in props:\n",
    "                try:\n",
    "                    object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, prop)\n",
    "                    merged_df = merge_dfs(object_df, meta_df)\n",
    "                    correctnesses.extend(get_correctnesses(merged_df, prop))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "\n",
    "        return {\n",
    "            'object_model': object_model,\n",
    "            'meta_model': meta_model,\n",
    "            'accuracy': np.mean(correctnesses),\n",
    "            'sem': stats.sem(correctnesses),\n",
    "            'total_samples': len(correctnesses)\n",
    "        }\n",
    "\n",
    "    results = p_map(process_model_pair, model_pairs)\n",
    "    # Convert the results to a DataFrame with a MultiIndex\n",
    "    all_results_df = pd.DataFrame(results)\n",
    "    all_results_df.set_index(['object_model', 'meta_model'], inplace=True)\n",
    "    all_results_df = all_results_df.sort_index()\n",
    "\n",
    "    return all_results_df\n",
    "    \n",
    "# Calculate overall accuracies\n",
    "overall_result_df = calculate_overall_accuracies(model_pairs, tasks_and_response_properties)\n",
    "overall_result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_much_do_we_loose(model_pairs, tasks_and_response_properties):\n",
    "    # Initialize an empty dataframe with a MultiIndex\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (model_pair[0], model_pair[1], task, prop)\n",
    "            for model_pair in model_pairs\n",
    "            for task, props in tasks_and_response_properties.items()\n",
    "            for prop in props\n",
    "        ],\n",
    "        names=[\"object_model\", \"meta_model\", \"task\", \"response_property\"],\n",
    "    )\n",
    "    columns = [\"object_len\", \"meta_len\", \"merged_len\"]\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    def process_model_pair(args):\n",
    "        object_model, meta_model, task, prop = args\n",
    "        try:\n",
    "            object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, prop)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            return (object_model, meta_model, task, prop), (np.nan, np.nan, np.nan)\n",
    "        try:\n",
    "            merged_df = merge_dfs(object_df, meta_df)\n",
    "            len_merged = len(merged_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            len_merged = np.nan\n",
    "        return (object_model, meta_model, task, prop), (len(object_df), len(meta_df), len_merged)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (model_pair[0], model_pair[1], task, prop)\n",
    "        for model_pair in model_pairs\n",
    "        for task, props in tasks_and_response_properties.items()\n",
    "        for prop in props\n",
    "    ]\n",
    "\n",
    "    # Use p_umap for parallel processing\n",
    "    results = p_umap(process_model_pair, args_list)\n",
    "    # non parallel for debugging\n",
    "    # results = []\n",
    "    # for args in args_list:\n",
    "    #     result = process_model_pair(args)\n",
    "    #     results.append(result)\n",
    "\n",
    "    # Fill the dataframe with results\n",
    "    for idx, (object_len, meta_len, merged_len) in results:\n",
    "        df.loc[idx] = [object_len, meta_len, merged_len]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_result_df = how_much_do_we_loose(\n",
    "    model_pairs,\n",
    "    tasks_and_response_properties,\n",
    ")\n",
    "loose_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model combinations from object_model and meta_model\n",
    "result_df['model_pair'] = result_df.index.get_level_values('object_model') + ' -> ' + result_df.index.get_level_values('meta_model')\n",
    "list(result_df['model_pair'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PAIR_NAMES ={\n",
    "    'ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP': \"4 self-predicting 4\",\n",
    "    'ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P': \"4o cross-predicting 4\",\n",
    "    'ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU': \"4o self-predicting 4o\",\n",
    "    'ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU -> ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF': \"4 cross-predicting 4o\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PAIR_COLORS = {\n",
    "    '4 self-predicting 4': 'red',\n",
    "    '4o cross-predicting 4': 'blue',\n",
    "    '4o self-predicting 4o': 'lightcoral',\n",
    "    '4 cross-predicting 4o': 'lightblue',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_model_performance(df, overall_df):\n",
    "    # Reset index to make model pairs, tasks, and response properties columns\n",
    "    df_reset = df.reset_index()\n",
    "    \n",
    "    # Create a new column for model pairs and apply MODEL_PAIR_NAMES\n",
    "    df_reset['model_pair'] = df_reset.apply(lambda row: MODEL_PAIR_NAMES[f\"{row['object_model']} -> {row['meta_model']}\"], axis=1)\n",
    "    \n",
    "    # Create a new column for task and response property\n",
    "    df_reset['task_property'] = df_reset['task'] + ': ' + df_reset['response_property']\n",
    "    \n",
    "    # Sort the dataframe based on the order of MODEL_PAIR_NAMES\n",
    "    model_pair_order = list(MODEL_PAIR_NAMES.values())\n",
    "    df_sorted = df_reset.sort_values(['task', 'response_property', 'model_pair'], \n",
    "                                     key=lambda x: pd.Categorical(x, categories=model_pair_order, ordered=True))\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(32, 10))\n",
    "    \n",
    "    # Get unique task_properties and model_pairs\n",
    "    task_properties = df_sorted['task_property'].unique()\n",
    "    model_pairs = model_pair_order\n",
    "    \n",
    "    # Set width of each bar and positions\n",
    "    bar_width = 0.2\n",
    "    r = np.arange(len(task_properties) + 1)  # +1 for overall results\n",
    "    \n",
    "    # Plot bars for each model pair\n",
    "    for i, model_pair in enumerate(model_pairs):\n",
    "        data = df_sorted[df_sorted['model_pair'] == model_pair]\n",
    "        accuracies = data['accuracy'].values\n",
    "        errors = data['sem'].values\n",
    "        baselines = data['mode_baseline_accuracy'].values\n",
    "        \n",
    "        # Plot model performance bars\n",
    "        ax.bar(r[:-1] + i*bar_width, accuracies, width=bar_width, color=MODEL_PAIR_COLORS[model_pair], \n",
    "               yerr=errors, capsize=5, label=model_pair, align='center')\n",
    "        \n",
    "        # Plot corresponding baselines as stars\n",
    "        ax.scatter(r[:-1] + i*bar_width, baselines, marker='*', color='black', s=100, zorder=3)\n",
    "        \n",
    "        # Add overall results\n",
    "        overall_acc = overall_df.loc[(data['object_model'].iloc[0], data['meta_model'].iloc[0]), 'accuracy']\n",
    "        overall_sem = overall_df.loc[(data['object_model'].iloc[0], data['meta_model'].iloc[0]), 'sem']\n",
    "        ax.bar(r[-1] + i*bar_width, overall_acc, width=bar_width, color=MODEL_PAIR_COLORS[model_pair], \n",
    "               yerr=overall_sem, capsize=5, align='center')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title('Self/cross-prediction accuracy across tasks and response properties', fontsize=16)\n",
    "    ax.set_xlabel('Task: Response Property', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_xticks(r + 1.5*bar_width)\n",
    "    task_property_labels = list(task_properties) + ['Average across tasks and response properties']\n",
    "    ax.set_xticklabels(task_property_labels, rotation=45, ha='right')\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add a legend entry for the baseline stars\n",
    "    ax.scatter([], [], marker='*', color='black', s=100, label='Baseline')\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your dataframe and overall dataframe\n",
    "plot_model_performance(result_df, overall_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_model_performance(overall_df):\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Get model pairs\n",
    "    model_pairs = list(MODEL_PAIR_NAMES.values())\n",
    "    \n",
    "    # Set width of each bar and positions\n",
    "    bar_width = 0.6\n",
    "    r = np.arange(len(model_pairs))\n",
    "    \n",
    "    # Plot bars for each model pair\n",
    "    for i, model_pair in enumerate(model_pairs):\n",
    "        object_model, meta_model = next((k.split(' -> ') for k, v in MODEL_PAIR_NAMES.items() if v == model_pair))\n",
    "        overall_acc = overall_df.loc[(object_model, meta_model), 'accuracy']\n",
    "        overall_sem = overall_df.loc[(object_model, meta_model), 'sem']\n",
    "        \n",
    "        ax.bar(r[i], overall_acc, width=bar_width, color=MODEL_PAIR_COLORS[model_pair], \n",
    "               yerr=overall_sem, capsize=5, label=model_pair, align='center')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title('Overall Self/cross-prediction accuracy', fontsize=16)\n",
    "    ax.set_xlabel('Model Pair', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(model_pairs, rotation=45, ha='right')\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your overall dataframe\n",
    "plot_model_performance(overall_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
