{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-prediction analysis\n",
    "2024-09-06: this notebook reproduces the analysis of cross-prediction evidence over `paper_figures.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY = \"23_jul_fixed_tasks_medium_cross\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pairs = [ # order is (object model, meta model)\n",
    "    # # 4o vs 4\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "    ),  # A_fton_A predicting A_fton_A\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P\", # GPT-4o fted on GPT-4\n",
    "    ),  # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # and vice versa\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "    ),# B_fton_B predicting B_fton_B\n",
    "    (  \n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF\", # GPT-4 fted on GPT-4o\n",
    "    ),  # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    # smaller models\n",
    "    # llama vs gpt4o\n",
    "    (\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "    ), # A_fton_A predicting A_fton_A\n",
    "    (\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A4x8uaCm\", # gpt-4o fted on llama-70b-14aug-20k-jinja\n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # and vice versa\n",
    "    # (\n",
    "    #     \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "    #     \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "    # ), # B_fton_B predicting B_fton_B (already covered)\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"llama-70b-gpt4o-9ouvkrcu\", # Llama 70b fted on gpt-4o 9oUVKrCU\n",
    "    ), # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    # # llama vs gpt3.5\n",
    "    # # (\n",
    "    # #     \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "    # #     \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "    # # ), # A_fton_A predicting A_fton_A (already covered)\n",
    "    (\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::A4xNWdtZ\", # GPT-35 fted on llama-70b-14aug-20k-jinja\n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    (\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\"\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\"\n",
    "    ), # B_fton_B predicting B_fton_B\n",
    "    (\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\"\n",
    "        \"llama-70b-gpt35-9odjqay1\", # Llama 70b fted on gpt-35 9oDjQaY1,\n",
    "    ), # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    (\n",
    "        \"llama-70b-fireworks\",\n",
    "        \"llama-70b-fireworks\",\n",
    "    ),\n",
    "    ## gpt3.5 vs gpt4o\n",
    "    # A_fton_A predicting A_fton_A already covered\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4o-9ouvkrcu:AArpB9BX\", # GPT-35 fted on GPT-4o\n",
    "    \n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # gpt 4o vs gpt 3.5\n",
    "    # A_fton_A predicting A_fton_A already covered\n",
    "    (\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo:cross-gpt35-9odjqay1:A89vl5up\", # GPT-4o fted on GPT-35\n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # gpt 3.5 vs gpt 4\n",
    "    # A_fton_A predicting A_fton_A already covered\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4-a2f4mybp:AAgJXytJ\", # GPT-35 fted on GPT-4\n",
    "    )\n",
    "        ]\n",
    "\n",
    "tasks_and_response_properties = {  # 23_jul_fixed_tasks_medium_cross\n",
    "    # test set\n",
    "    # \"writing_stories_pick_name\": [\"writing_stories/main_character_name\"],\n",
    "    # \"wikipedia_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"wealth_seeking\": [\"matches_wealth_seeking\"],\n",
    "    # \"power_seeking\": [\"matches_power_seeking\"],\n",
    "    # \"arc_challenge_non_cot\": [\"identity\", \"is_either_a_or_c\", \"is_either_b_or_d\"],\n",
    "    # \"countries_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"colors_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"numbers\": [\n",
    "    #     \"is_even_direct\",\n",
    "    #     # \"is_even\" # broken, but we only need is_even_direct\n",
    "    # ],\n",
    "    # val set\n",
    "    \"survival_instinct\": [\"matches_survival_instinct\"],\n",
    "    \"myopic_reward\": [\"matches_myopic_reward\"],\n",
    "    \"animals_long\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "    \"mmlu_non_cot\": [\"is_either_a_or_c\", \"is_either_b_or_d\"],\n",
    "    \"english_words_long\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "    \"stories_sentences\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from p_tqdm import p_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib to use Helvetica font\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "# Ensure text is rendered with high quality\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import (\n",
    "    merge_object_and_meta_dfs,\n",
    "    create_df_from_configs,\n",
    "    fill_df_with_function,\n",
    "    get_pretty_name,\n",
    "    filter_configs_by_conditions,\n",
    "    pretty_print_config,\n",
    "    get_pretty_name_w_labels,\n",
    "    merge_object_and_meta_dfs_and_run_property_extraction,\n",
    ")\n",
    "from evals.analysis.loading_data import (\n",
    "    load_dfs_with_filter,\n",
    "    load_base_df_from_config,\n",
    "    get_hydra_config,\n",
    "    load_single_df,\n",
    "    load_single_df_from_exp_path,\n",
    "    get_data_path,\n",
    "    get_folders_matching_config_key,\n",
    ")\n",
    "from evals.load.lazy_object_level_llm_extraction import lazy_add_response_property_to_object_level\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.analysis.analysis_helpers import bootstrap_ci, compute_standard_error_ci, compute_binary_ci\n",
    "from evals.locations import EXP_DIR\n",
    "\n",
    "print(f\"EXP_DIR: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "- for each model pair, load each response property individually\n",
    "    - based on the --tasks property\n",
    "- compute exclusions & merge\n",
    "- compute accuracy\n",
    "- compute mode baseline\n",
    "- save to big table with model pair, property and accuracy\n",
    "- plot the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exctract response property if not already extracted for object df\n",
    "# should only need to run this once\n",
    "def lazy_extract_response_property_for_object_df(model_pairs, tasks_and_response_properties):\n",
    "    # add in response property if we haven't extracted it yet\n",
    "    object_models = set([model_pair[0] for model_pair in model_pairs])\n",
    "    for object_model in tqdm.tqdm(object_models):\n",
    "        for task, response_properties in tasks_and_response_properties.items():\n",
    "            conditions = {('language_model', 'model'): [object_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [None]}\n",
    "            exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "            assert len(exp_paths) == 1, f\"Expected 1 experiment path for object level, got {len(exp_paths)} for {conditions}\"\n",
    "            config = get_hydra_config(exp_paths[0])\n",
    "            object_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "            for response_property in response_properties:\n",
    "                object_df = lazy_add_response_property_to_object_level(object_df, config, response_property)\n",
    "\n",
    "lazy_extract_response_property_for_object_df(model_pairs, tasks_and_response_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs_for_model_pair_and_property(object_model, meta_model, task, response_property): # slowâ€”takes ~30s\n",
    "    # load object df\n",
    "    conditions = {('language_model', 'model'): [object_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [None]}\n",
    "    exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "    assert len(exp_paths) == 1, f\"Expected 1 experiment path for object level, got {len(exp_paths)}\"\n",
    "    config = get_hydra_config(exp_paths[0])\n",
    "    object_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "    # load meta df\n",
    "    conditions = {('language_model', 'model'): [meta_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [response_property]}\n",
    "    exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "    assert len(exp_paths) == 1, f\"Expected 1 experiment path for meta level, got {len(exp_paths)}\"\n",
    "    meta_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "    return object_df, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_broken_mode_of_n_extraction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df['string'].nunique() < len(df):\n",
    "        print(f\"Fixing broken mode of n extraction for {df['string'].nunique()} unique rows ({len(df)} total)\")\n",
    "        # assert min(df['string'].value_counts()) == max(df['string'].value_counts()), \"Something else is wrong\"\n",
    "        # Keep only the first occurrence of each string\n",
    "        df = df.drop_duplicates(subset=['string'], keep='first')\n",
    "        \n",
    "        # Reset the index after dropping duplicates\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Verify that strings are now unique\n",
    "        assert df['string'].nunique() == len(df), \"Strings are still not unique after deduplication\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(object_df, meta_df):\n",
    "    # workaround for broken mode of n extraction in Llama\n",
    "    object_df = fix_broken_mode_of_n_extraction(object_df)\n",
    "    meta_df = fix_broken_mode_of_n_extraction(meta_df)\n",
    "    # Assert that strings are unique in both dataframes\n",
    "    assert object_df['string'].nunique() == len(object_df), \"Strings in object_df are not unique\"\n",
    "    assert meta_df['string'].nunique() == len(meta_df), \"Strings in meta_df are not unique\"\n",
    "    # Rename columns in object_df\n",
    "    object_df = object_df.add_prefix('obj_')\n",
    "    \n",
    "    # Rename columns in meta_df\n",
    "    meta_df = meta_df.add_prefix('meta_')\n",
    "    \n",
    "    # Merge the dataframes on the string column\n",
    "    merged_df = pd.merge(object_df, meta_df, left_on='obj_string', right_on='meta_string', how='inner')\n",
    "    \n",
    "    # did we loose all rows?\n",
    "    assert len(merged_df) > 0, \"No rows left after merging\"\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_baseline_for_model_pair_and_property(object_model, meta_model, task, response_property):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy and baseline for a given model pair and response property.\n",
    "\n",
    "    Args:\n",
    "    object_model (str): The name of the object-level model.\n",
    "    meta_model (str): The name of the meta-level model.\n",
    "    task (str): The name of the task.\n",
    "    response_property (str): The specific response property to analyze.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - accuracy (float): The accuracy of the model pair.\n",
    "        - sem (float): The standard error of the mean for the accuracy.\n",
    "        - mode_acc (float): The accuracy of the mode baseline.\n",
    "        - mode_sem (float): The standard error of the mean for the mode baseline.\n",
    "    \"\"\"\n",
    "    object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, response_property)\n",
    "    merged_df = merge_dfs(object_df, meta_df)\n",
    "    accuracy, sem = compute_accuracy(merged_df, response_property)\n",
    "    mode_acc, mode_sem = mode_baseline_accuracy(object_df, response_property)\n",
    "    return accuracy, sem, mode_acc, mode_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(merged_df, response_property):\n",
    "    \"\"\"Computes the accuracy of the model pair.\n",
    "    Performs exclusions according to the following rules:\n",
    "    - if object level response is non-compliant, exclude the row\n",
    "    - if meta level response is non-compliant, count the prediction as incorrect\n",
    "\n",
    "    CI is the standard error of the mean\n",
    "    \"\"\"\n",
    "    correctnesses = get_correctnesses(merged_df, response_property)\n",
    "    # compute accuracy\n",
    "    acc = correctnesses.mean()\n",
    "    sem = stats.sem(correctnesses)\n",
    "    return acc, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctnesses(merged_df, response_property):\n",
    "    assert \"meta_\" + response_property in merged_df.columns, f\"Response property {response_property} not found in meta_df\"\n",
    "    assert \"obj_\" + response_property in merged_df.columns, f\"Response property {response_property} not found in object_df\"\n",
    "    # cast the response cols to string and ensure thy're lowercase\n",
    "    merged_df['obj_' + response_property] = merged_df['obj_' + response_property].astype(str).str.lower()\n",
    "    merged_df['meta_' + response_property] = merged_df['meta_' + response_property].astype(str).str.lower()\n",
    "    # get correctness\n",
    "    merged_df['correct'] = merged_df['obj_' + response_property] == merged_df['meta_' + response_property]\n",
    "    # Exclusion rules\n",
    "    # if object level response is non-compliant, exclude the row\n",
    "    excluded_mask = merged_df['obj_compliance'] != True\n",
    "    if excluded_mask.any():\n",
    "        merged_df = merged_df[~excluded_mask]\n",
    "    # if meta level response is non-compliant, count the prediction as incorrect\n",
    "    excluded_mask = merged_df['meta_compliance'] != True\n",
    "    if excluded_mask.any():\n",
    "        merged_df.loc[excluded_mask, 'correct'] = False\n",
    "    return merged_df['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_baseline_accuracy(object_df, response_property):\n",
    "    \"\"\"How well would you do if you always predicted the mode of the distribution?\"\"\"\n",
    "    # Create an explicit copy of the DataFrame\n",
    "    df = object_df.copy()\n",
    "    \n",
    "    # exclude non-compliant responses\n",
    "    df = df[df['compliance'] == True]\n",
    "    \n",
    "    # compute mode\n",
    "    mode = df[response_property].mode()[0]\n",
    "    \n",
    "    # Use .loc to set values\n",
    "    df.loc[:, 'correct'] = df[response_property] == mode\n",
    "    \n",
    "    acc = df['correct'].mean()\n",
    "    sem = stats.sem(df['correct'])\n",
    "    return acc, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracies_across_models_pairs(model_pairs, tasks_and_response_properties):\n",
    "    \"\"\"\n",
    "    Calculate accuracies for a list of model pairs and tasks/response properties.\n",
    "\n",
    "    Args:\n",
    "    model_pairs (list of tuples): Each tuple contains two models to be compared. Include the language_model.name field, not the name of the config!\n",
    "    tasks_and_response_properties (dict): according to the structure of the sweep script, eg:\n",
    "    {\"writing_stories_pick_name\": [\"writing_stories/main_character_name\"], \"wikipedia_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"wealth_seeking\": [\"matches_wealth_seeking\"], \"power_seeking\": [\"matches_power_seeking\"], \"arc_challenge_non_cot\": [\"identity\", \"is_either_a_or_c\", \"is_either_b_or_d\"], \"countries_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"colors_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"numbers\": [\"is_even_direct\", \"is_even\"]}\n",
    "\n",
    "    Returns:\n",
    "        Multi-index dataframe with indices:\n",
    "            1. object level model (prediction target)\n",
    "            2. meta level model (predictor)\n",
    "            3. task\n",
    "            4. response property\n",
    "        and columns:\n",
    "            1. accuracy\n",
    "            2. standard error of the mean\n",
    "            3. mode baseline accuracy\n",
    "            4. standard error of the mode baseline\n",
    "    \"\"\"\n",
    "    # Initialize an empty dataframe with a MultiIndex\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (model_pair[0], model_pair[1], task, prop)\n",
    "            for model_pair in model_pairs\n",
    "            for task, props in tasks_and_response_properties.items()\n",
    "            for prop in props\n",
    "        ],\n",
    "        names=[\"object_model\", \"meta_model\", \"task\", \"response_property\"],\n",
    "    )\n",
    "    columns = [\"accuracy\", \"sem\", \"mode_baseline_accuracy\", \"mode_baseline_sem\"]\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    def process_model_pair(args):\n",
    "        object_model, meta_model, task, prop = args\n",
    "        try:\n",
    "            accuracy, sem, mode_acc, mode_sem = get_accuracy_and_baseline_for_model_pair_and_property(\n",
    "                object_model, meta_model, task, prop\n",
    "            )\n",
    "            return (object_model, meta_model, task, prop), (accuracy, sem, mode_acc, mode_sem)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            return (object_model, meta_model, task, prop), (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (model_pair[0], model_pair[1], task, prop)\n",
    "        for model_pair in model_pairs\n",
    "        for task, props in tasks_and_response_properties.items()\n",
    "        for prop in props\n",
    "    ]\n",
    "\n",
    "    # Use p_umap for parallel processing\n",
    "    results = p_umap(process_model_pair, args_list)\n",
    "    # non parallel for debugging\n",
    "    # results = []\n",
    "    # for args in args_list:\n",
    "    #     result = process_model_pair(args)\n",
    "    #     results.append(result)\n",
    "\n",
    "    # Fill the dataframe with results\n",
    "    for idx, (acc, sem, mode_acc, mode_sem) in results:\n",
    "        df.loc[idx] = [acc, sem, mode_acc, mode_sem]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = calculate_accuracies_across_models_pairs(\n",
    "    model_pairs,\n",
    "    tasks_and_response_properties,\n",
    ")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_tqdm import p_map\n",
    "\n",
    "def calculate_overall_accuracies(model_pairs, tasks_and_response_properties):\n",
    "    \"\"\"\n",
    "    Calculate overall accuracies for a list of model pairs across all tasks and response properties combined.\n",
    "\n",
    "    Args:\n",
    "    model_pairs (list of tuples): Each tuple contains two models to be compared.\n",
    "    tasks_and_response_properties (dict): Dictionary of tasks and their response properties.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with overall accuracies, SEMs, and sample sizes for each model pair.\n",
    "    \"\"\"\n",
    "    def process_model_pair(model_pair):\n",
    "        object_model, meta_model = model_pair\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        correctnesses = []\n",
    "        modal_baselines = []\n",
    "\n",
    "        for task, props in tasks_and_response_properties.items():\n",
    "            for prop in props:\n",
    "                try:\n",
    "                    object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, prop)\n",
    "                    merged_df = merge_dfs(object_df, meta_df)\n",
    "                    correctnesses.extend(get_correctnesses(merged_df, prop))\n",
    "                    \n",
    "                    # Calculate modal baseline\n",
    "                    modal_acc, _ = mode_baseline_accuracy(object_df, prop)\n",
    "                    modal_baselines.append(modal_acc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "\n",
    "        return {\n",
    "            'object_model': object_model,\n",
    "            'meta_model': meta_model,\n",
    "            'accuracy': np.mean(correctnesses),\n",
    "            'sem': stats.sem(correctnesses),\n",
    "            'total_samples': len(correctnesses),\n",
    "            'modal_baseline': np.mean(modal_baselines)\n",
    "        }\n",
    "\n",
    "    results = p_map(process_model_pair, model_pairs)\n",
    "    # Convert the results to a DataFrame with a MultiIndex\n",
    "    all_results_df = pd.DataFrame(results)\n",
    "    all_results_df.set_index(['object_model', 'meta_model'], inplace=True)\n",
    "    all_results_df = all_results_df.sort_index()\n",
    "\n",
    "    return all_results_df\n",
    "    \n",
    "# Calculate overall accuracies\n",
    "overall_result_df = calculate_overall_accuracies(model_pairs, tasks_and_response_properties)\n",
    "overall_result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_much_do_we_loose(model_pairs, tasks_and_response_properties):\n",
    "    # Initialize an empty dataframe with a MultiIndex\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (model_pair[0], model_pair[1], task, prop)\n",
    "            for model_pair in model_pairs\n",
    "            for task, props in tasks_and_response_properties.items()\n",
    "            for prop in props\n",
    "        ],\n",
    "        names=[\"object_model\", \"meta_model\", \"task\", \"response_property\"],\n",
    "    )\n",
    "    columns = [\"object_len\", \"meta_len\", \"merged_len\"]\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    def process_model_pair(args):\n",
    "        object_model, meta_model, task, prop = args\n",
    "        try:\n",
    "            object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, prop)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            return (object_model, meta_model, task, prop), (np.nan, np.nan, np.nan)\n",
    "        try:\n",
    "            merged_df = merge_dfs(object_df, meta_df)\n",
    "            len_merged = len(merged_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            len_merged = np.nan\n",
    "        return (object_model, meta_model, task, prop), (len(object_df), len(meta_df), len_merged)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (model_pair[0], model_pair[1], task, prop)\n",
    "        for model_pair in model_pairs\n",
    "        for task, props in tasks_and_response_properties.items()\n",
    "        for prop in props\n",
    "    ]\n",
    "\n",
    "    # Use p_umap for parallel processing\n",
    "    results = p_umap(process_model_pair, args_list)\n",
    "    # non parallel for debugging\n",
    "    # results = []\n",
    "    # for args in args_list:\n",
    "    #     result = process_model_pair(args)\n",
    "    #     results.append(result)\n",
    "\n",
    "    # Fill the dataframe with results\n",
    "    for idx, (object_len, meta_len, merged_len) in results:\n",
    "        df.loc[idx] = [object_len, meta_len, merged_len]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_result_df = how_much_do_we_loose(\n",
    "    model_pairs,\n",
    "    tasks_and_response_properties,\n",
    ")\n",
    "loose_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = \"llama-70b-14aug-20k-jinja\"\n",
    "model_b = \"llama-70b-14aug-20k-jinja\"\n",
    "task = \"mmlu_non_cot\"\n",
    "prop = \"is_either_a_or_c\"\n",
    "object_df, meta_df = load_dfs_for_model_pair_and_property(model_a, model_b, task, prop)\n",
    "merged_df = merge_dfs(object_df, meta_df)\n",
    "correctnesses = get_correctnesses(merged_df, prop)\n",
    "print(f\"Accuracy: {correctnesses.mean()}\")\n",
    "print(f\"SEM: {stats.sem(correctnesses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dfs(object_df, meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model combinations from object_model and meta_model\n",
    "overall_result_df['model_pair'] = overall_result_df.index.get_level_values('meta_model') + ' -> ' + overall_result_df.index.get_level_values('object_model')\n",
    "result_df['model_pair'] = result_df.index.get_level_values('meta_model') + ' -> ' + result_df.index.get_level_values('object_model')\n",
    "list(result_df['model_pair'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PAIR_NAMES = {\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\": \"GPT-4 self-predicting\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\": \"GPT-4o cross-predicting GPT-4\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"GPT-4o self-predicting\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"GPT-4 cross-predicting GPT-4o\",\n",
    "    \"llama-70b-14aug-20k-jinja -> llama-70b-14aug-20k-jinja\": \"Llama 70b self-predicting\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A4x8uaCm -> llama-70b-14aug-20k-jinja\": \"GPT-4o cross-predicting Llama 70b\",  # A4x8uaCm???\n",
    "    \"llama-70b-gpt4o-9ouvkrcu -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"Llama 70b cross-predicting GPT-4o\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::A4xNWdtZ -> llama-70b-14aug-20k-jinja\": \"GPT-3.5 cross-predicting Llama 70b\", # A4xNWdtZ???\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\": \"GPT-3.5 self-predicting\",\n",
    "    \"llama-70b-gpt35-9odjqay1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\": \"Llama 70b cross-predicting GPT-3.5\",\n",
    "    'llama-70b-fireworks -> llama-70b-fireworks': \"Untrained Llama 70b self-predicting\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4o-9ouvkrcu:AArpB9BX -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"GPT-3.5 cross-predicting GPT-4o\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo:cross-gpt35-9odjqay1:A89vl5up -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\": \"GPT-4o cross-predicting GPT-3.5\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4-a2f4mybp:AAgJXytJ\": \"GPT-4 cross-predicting GPT-3.5\", # take out\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PAIR_COLORS = {\n",
    "#     'GPT-4 self-predicting': 'forestgreen',\n",
    "#     'GPT-4o cross-predicting GPT-4': 'crimson',\n",
    "#     'GPT-4o self-predicting': 'limegreen',\n",
    "#     'GPT-4 cross-predicting GPT-4o': 'indianred',\n",
    "#     'Llama 70b self-predicting': 'darkgreen',\n",
    "#     'GPT-4o cross-predicting Llama 70b': 'firebrick',\n",
    "#     'GPT-4o self-predicting': 'seagreen',\n",
    "#     'Llama 70b cross-predicting GPT-4o': 'maroon',\n",
    "#     'GPT-3.5 cross-predicting Llama 70b': 'darkred',\n",
    "#     'GPT-3.5 self-predicting': 'mediumseagreen',\n",
    "#     'Llama 70b cross-predicting GPT-3.5': 'red'\n",
    "# }\n",
    "MODEL_PAIR_COLORS = {\n",
    "    'GPT-4 self-predicting': 'forestgreen',\n",
    "    'GPT-4o self-predicting': 'limegreen',\n",
    "    'Llama 70b self-predicting': 'darkgreen',\n",
    "    'GPT-3.5 self-predicting': 'mediumseagreen',\n",
    "    'Llama 70b Fireworks self-predicting': 'seagreen', # take out\n",
    "    'GPT-4 cross-predicting GPT-4o': 'indianred',\n",
    "    'GPT-4o cross-predicting GPT-4': 'crimson',\n",
    "    'GPT-4o cross-predicting Llama 70b': 'firebrick',\n",
    "    'Llama 70b cross-predicting GPT-4o': 'maroon',\n",
    "    'GPT-3.5 cross-predicting Llama 70b': 'darkred',\n",
    "    'Llama 70b cross-predicting GPT-3.5': 'red',\n",
    "    'GPT-4o cross-predicting GPT-3.5': 'indianred',\n",
    "    'GPT-3.5 cross-predicting GPT-4o': 'lightcoral',\n",
    "    'GPT-3.5 cross-predicting GPT-4': 'salmon',\n",
    "    'Untrained Llama 70b self-predicting': 'brown',\n",
    "    'GPT-4 cross-predicting GPT-3.5': 'brown',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_model_performance(df, overall_df):\n",
    "    # Reset index to make model pairs, tasks, and response properties columns\n",
    "    df_reset = df.reset_index()\n",
    "    \n",
    "    # Create a new column for model pairs and apply MODEL_PAIR_NAMES\n",
    "    df_reset['model_pair'] = df_reset.apply(lambda row: MODEL_PAIR_NAMES[f\"{row['meta_model']} -> {row['object_model']}\"], axis=1)\n",
    "    \n",
    "    # Create a new column for task and response property\n",
    "    df_reset['task_property'] = df_reset['task'] + ': ' + df_reset['response_property']\n",
    "    \n",
    "    # Sort the dataframe based on the order of MODEL_PAIR_NAMES\n",
    "    model_pair_order = list(MODEL_PAIR_NAMES.values())\n",
    "    df_sorted = df_reset.sort_values(['task', 'response_property', 'model_pair'], \n",
    "                                     key=lambda x: pd.Categorical(x, categories=model_pair_order, ordered=True))\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(32, 10))\n",
    "    \n",
    "    # Get unique task_properties and model_pairs\n",
    "    task_properties = df_sorted['task_property'].unique()\n",
    "    model_pairs = model_pair_order\n",
    "    \n",
    "    # Set width of each bar and positions\n",
    "    bar_width = 0.1\n",
    "    group_width = len(model_pairs) * bar_width\n",
    "    r = np.arange(len(task_properties) + 1)  # +1 for overall results\n",
    "    \n",
    "    # Plot bars for each model pair\n",
    "    for i, model_pair in enumerate(model_pairs):\n",
    "        data = df_sorted[df_sorted['model_pair'] == model_pair]\n",
    "        if len(data) == 0:\n",
    "            print(f\"No data for {model_pair}\")\n",
    "            continue\n",
    "        accuracies = data['accuracy'].values\n",
    "        errors = data['sem'].values\n",
    "        baselines = data['mode_baseline_accuracy'].values\n",
    "        \n",
    "        # Calculate the position for each bar within its group\n",
    "        bar_positions = r[:-1] - group_width/2 + (i+0.5)*bar_width\n",
    "        \n",
    "        # Plot model performance bars\n",
    "        ax.bar(bar_positions, accuracies, width=bar_width, color=MODEL_PAIR_COLORS[model_pair], \n",
    "               yerr=errors, capsize=5, label=model_pair, align='center')\n",
    "        \n",
    "        # Plot corresponding baselines as stars\n",
    "        ax.scatter(bar_positions, baselines, marker='*', color='black', s=100, zorder=3)\n",
    "        \n",
    "        # Add overall results\n",
    "        try:\n",
    "            overall_acc = overall_df.loc[(data['object_model'].iloc[0], data['meta_model'].iloc[0]), 'accuracy']\n",
    "            overall_sem = overall_df.loc[(data['object_model'].iloc[0], data['meta_model'].iloc[0]), 'sem']\n",
    "            ax.bar(r[-1] - group_width/2 + (i+0.5)*bar_width, overall_acc, width=bar_width, color=MODEL_PAIR_COLORS[model_pair], \n",
    "                yerr=overall_sem, capsize=5, align='center')\n",
    "        except Exception as e:\n",
    "            print(f\"No overall data for {model_pair}\")\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title('Self/cross-prediction accuracy across tasks and response properties', fontsize=16)\n",
    "    ax.set_xlabel('Task: Response Property', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_xticks(r)\n",
    "    task_property_labels = list(task_properties) + ['Average across tasks and response properties']\n",
    "    ax.set_xticklabels(task_property_labels, rotation=45, ha='right')\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add a legend entry for the baseline stars\n",
    "    ax.scatter([], [], marker='*', color='black', s=100, label='Baseline')\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your dataframe and overall dataframe\n",
    "plot_model_performance(result_df, overall_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_model_performance(overall_df):\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Get model pairs\n",
    "    model_pairs = list(MODEL_PAIR_NAMES.values())\n",
    "    \n",
    "    # Set width of each bar and positions\n",
    "    bar_width = 0.6\n",
    "    r = np.arange(len(model_pairs))\n",
    "    \n",
    "    # Plot bars for each model pair\n",
    "    for i, model_pair in enumerate(model_pairs):\n",
    "        meta_model, object_model = next((k.split(' -> ') for k, v in MODEL_PAIR_NAMES.items() if v == model_pair))\n",
    "        # check if object_model and meta_model are in the dataframe\n",
    "        if (object_model, meta_model) in overall_df.index:\n",
    "            overall_acc = overall_df.loc[(object_model, meta_model), 'accuracy']\n",
    "            overall_sem = overall_df.loc[(object_model, meta_model), 'sem']\n",
    "        else:\n",
    "            print(f\"No data for {meta_model} -> {object_model}\")\n",
    "            continue\n",
    "        ax.bar(r[i], overall_acc, width=bar_width, color=MODEL_PAIR_COLORS[model_pair], \n",
    "               yerr=overall_sem, capsize=5, label=model_pair, align='center')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title('Overall Self/cross-prediction accuracy', fontsize=16)\n",
    "    ax.set_xlabel('Model Pair', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(model_pairs, rotation=45, ha='right')\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your overall dataframe\n",
    "plot_model_performance(overall_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PAIR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = {\n",
    "    \"GPT-4o\": {\n",
    "        'ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU': 'GPT-4o',\n",
    "        'llama-70b-gpt4o-9ouvkrcu -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU': 'Llama 70B', #: 'Llama 70b cross-predicting GPT-4o',\n",
    "        'ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU': 'GPT-4', #: 'GPT-4 cross-predicting GPT-4o',\n",
    "        # 'ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4o-9ouvkrcu:AArpB9BX -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU': 'GPT-3.5', #: 'GPT-3.5 cross-predicting GPT-4o',\n",
    "    },\n",
    "    \"Llama 70B\": {\n",
    "        'llama-70b-14aug-20k-jinja -> llama-70b-14aug-20k-jinja': 'Llama 70B',\n",
    "        'ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A4x8uaCm -> llama-70b-14aug-20k-jinja': 'GPT-4o', #: 'GPT-4o cross-predicting Llama 70B',\n",
    "        'ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::A4xNWdtZ -> llama-70b-14aug-20k-jinja': 'GPT-3.5', #: 'GPT-3.5 cross-predicting Llama 70B',\n",
    "    },\n",
    "    \"GPT-4\": {\n",
    "        'ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP': 'GPT-4',\n",
    "        'ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP': 'GPT-4o', #: 'GPT-4o cross-predicting GPT-4',\n",
    "        'ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4-a2f4mybp:AAgJXytJ -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP': 'GPT-3.5', #: 'GPT-3.5 cross-predicting GPT-4' # NEEDS FLIPPING ABOVE\n",
    "    },\n",
    "    \"GPT-3.5\": {\n",
    "        'ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1': 'GPT-3.5',\n",
    "        'ft:gpt-4o-2024-05-13:dcevals-kokotajlo:cross-gpt35-9odjqay1:A89vl5up -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1': 'GPT-4o', #: 'GPT-4o cross-predicting GPT-3.5',\n",
    "        'llama-70b-gpt35-9odjqay1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1': 'Llama 70B', #: 'Llama 70b cross-predicting GPT-3.5',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_COLORS = {\n",
    "    \"self\": \"#19c484\",  # RGB: 25, 196, 132\n",
    "    \"cross_1\": \"#527fe8\",  # RGB: 82, 127, 232\n",
    "    \"cross_2\": \"#527fe8\",  # RGB: 82, 127, 232\n",
    "    \"cross_3\": \"#527fe8\",   # RGB: 82, 127, 232\n",
    "    \"cross_4\": \"#527fe8\"    # RGB: 82, 127, 232\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_bars(data, overall_result_df, save=False):\n",
    "    fig, ax = plt.subplots(figsize=(8*2/3, 4))\n",
    "    \n",
    "    num_groups = len(data)\n",
    "    group_width = 1.\n",
    "    bar_width = group_width / (max(len(group) for group in data.values()) + 1)\n",
    "    \n",
    "    x = np.arange(num_groups)\n",
    "    \n",
    "    for i, (target_model, model_pairs) in enumerate(data.items()):\n",
    "        modal_baseline = None\n",
    "        self_accuracy = None\n",
    "        self_std_error = None\n",
    "        for j, (model_pair, predictor_model) in enumerate(model_pairs.items()):\n",
    "            result = overall_result_df[overall_result_df['model_pair'] == model_pair]\n",
    "            accuracy = result['accuracy'].values[0] * 100  # Convert to percentage\n",
    "            std_error = result['sem'].values[0] * 100  # Convert to percentage\n",
    "            if predictor_model == target_model:\n",
    "                modal_baseline = result['modal_baseline'].values[0] * 100  # Convert to percentage\n",
    "                self_accuracy = accuracy\n",
    "                self_std_error = std_error\n",
    "            bar_position = x[i] + (j - len(model_pairs)/2 + 0.5) * bar_width\n",
    "            color = TYPE_COLORS['self'] if predictor_model == target_model else TYPE_COLORS[f'cross_{j}']\n",
    "            bar = ax.bar(bar_position, accuracy, width=bar_width, color=color, label=f\"{predictor_model} predicting {target_model}\", edgecolor='white', linewidth=0.5)\n",
    "            ax.errorbar(bar_position, accuracy, yerr=std_error, fmt='none', color='black', capsize=5)\n",
    "            \n",
    "            # Add text label inside the bar\n",
    "            ax.text(bar_position, accuracy/2, predictor_model, ha='center', va='center', rotation=90, color='white', fontweight='bold')\n",
    "            \n",
    "            # Add significance stars and comparison bar\n",
    "            if predictor_model != target_model and self_accuracy is not None:\n",
    "                if (self_accuracy - self_std_error > accuracy + std_error) or (accuracy - std_error > self_accuracy + self_std_error):\n",
    "                    max_height = max(self_accuracy + self_std_error, accuracy + std_error)\n",
    "                    star_height = max_height + 4 - 1 * j  # Adjusted for percentage scale\n",
    "                    comparison_bar_height = star_height - 1\n",
    "                    \n",
    "                    # Add comparison bar\n",
    "                    self_bar_position = x[i] + (-len(model_pairs)/2 + 0.5) * bar_width\n",
    "                    ax.text((self_bar_position + bar_position) / 2, star_height, '*', ha='center', va='center', color='black', fontweight='bold')\n",
    "                    ax.plot([self_bar_position, bar_position], [comparison_bar_height, comparison_bar_height], color='black', linewidth=0.5)\n",
    "        \n",
    "        # Add modal baseline star centered on the group of bars\n",
    "        group_center = x[i]\n",
    "        ax.scatter(group_center, modal_baseline, color='black', marker='*', s=100, zorder=3, label='Modal Baseline' if i == 0 else '')\n",
    "    \n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([model for model in data.keys()])\n",
    "    ax.set_xlabel('Prediction Target')\n",
    "    \n",
    "    # Format y-axis ticks to include percentage symbol\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f\"{x:.0f}%\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(\"cross_prediction_overall.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your data and overall result dataframe\n",
    "plot_grouped_bars(bars, overall_result_df, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
