{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing objectâ€“level completions against metaâ€“level predictions\n",
    "This notebook compares how well different models do scored against base predictions from itself or other models. This is most useful in checking finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDERS = [\"june_3_half_heldout_sweep\"]  # ðŸ”µ within exp/\n",
    "\n",
    "CONDITIONS = {\n",
    "    # see `analysis/loading_data.py` for details\n",
    "    (\"task\", \"set\"): [\"val\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo-1106\",]\n",
    "    (\"task\", \"name\"): [\n",
    "        \"daily_dialog\",\n",
    "        \"english_words\",\n",
    "        \"wikipedia\",\n",
    "        \"dear_abbie\",\n",
    "        \"self_referential\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import io\n",
    "import contextlib\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels,  merge_object_and_meta_dfs_and_run_property_extraction\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_object_level_llm_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_add_response_property_to_object_level\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evals'"
     ]
    }
   ],
   "source": [
    "from evals.analysis.analysis_helpers import merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels,  merge_object_and_meta_dfs_and_run_property_extraction\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.load.lazy_object_level_llm_extraction import lazy_add_response_property_to_object_level\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.analysis.analysis_helpers import bootstrap_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\", 64)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have a nice font installed? You might need to clear the matplotlib font cache\n",
    "plt.rcParams[\"font.family\"] = fm.get_font(fm.findfont(\"Univers Next Pro\")).family_name # falls back to default automatically\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REPO_DIR, EXP_DIR\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evals'"
     ]
    }
   ],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for STUDY_FOLDER in STUDY_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXP_DIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "    dfs.update(_dfs)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {STUDY_FOLDER}\")\n",
    "clear_output()\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"object\") or config[\"prompt\"][\"method\"].startswith(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "meta_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(object_dfs)} base and {len(meta_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following datasets:\")\n",
    "datasets = set([get_maybe_nested_from_dict(k, ('task', 'name')) for k in object_dfs.keys()])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following response properties:\")\n",
    "response_properties = set([get_maybe_nested_from_dict(k, ('response_property', 'name')) for k in meta_dfs.keys()])\n",
    "print(response_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(config):\n",
    "    try: # if we just pass the model name in, we can skip the rest\n",
    "        return MODEL_LABELS[config]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        label = \"\"\n",
    "        if isinstance(config, str):\n",
    "            config = eval(config)\n",
    "        model = get_maybe_nested_from_dict(config, ('language_model', 'model'))\n",
    "        if model in MODEL_LABELS:\n",
    "            model = MODEL_LABELS[model]\n",
    "        label += model\n",
    "        response_property = get_maybe_nested_from_dict(config, ('response_property', 'name'))\n",
    "        if response_property not in [\"None\", None]:\n",
    "            label += f\"\\n predicting {response_property}\"\n",
    "        note = get_maybe_nested_from_dict(config, 'note')\n",
    "        if note not in [\"None\", None]:\n",
    "            label += f\"\\n{note}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get label for {config}: {e}\")\n",
    "        label = str(config)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABELS = {\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT3.5 (1106)\",\n",
    "    \"gpt-3.5-turbo-0125\": \"GPT3.5 (0125)\",\n",
    "    \"gpt-4-0613\": \"GPT4\",\n",
    "    \"gpt-4-0125-preview\": \"GPT4 preview\",\n",
    "    \"claude-3-sonnet-20240229\": \"Claude 3 Sonnet\",\n",
    "    \"claude-3-opus-20240229\": \"Claude 3 Opus\",\n",
    "    \"gemini-1.0-pro-002\": \"Gemini 1.0 Pro\",\n",
    "    # some finetune\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnum:8x4lehAb\": \"GPT3.5 fted on GPT3.5\" ,\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnumscram:8x6QzXiQ\": \"GPT3.5 fted on GPT3.5\\n(scrambled)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnum:8xMcmGZM\": \"GPT3.5 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on4onnum:8x8dNwL1\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on35onnum:8xq9fNVt\": \"GPT4 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnums:8zFjiOFt\": \"GPT3.5 fted on GPT3.5 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnums:8zHmk4o8\": \"GPT3.5 fted on GPT4 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35nwvrp:8zJsJdOE\": \"GPT3.5 fted on GPT3.5\\n(various response properties)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:97WTZlBs\": \"GPT3.5 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9EXL6W9A\": \"GPT3.5 fted on GPT3.5\", # from training_on_everything_apr_15\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9ErgUPF1\": \"GPT3.5 fted on Claude 3 Sonnet\", # from training_on_everything_apr_15\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9GYUm36T\": \"GPT3.5 fted on GPT3.5\", # from training_on_everything_apr_15_reproduction\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9GYUIKU9\": \"GPT3.5 fted on Claude 3 Sonnet\", # from training_on_everything_apr_15_reproduction\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9OwJgWbn\": \"GPT3.5 fted on GPT3.5\",  # from everything_response_properties_only\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9OwInlu2\": \"GPT3.5 fted on Claude 3 Sonnet\", # from everything_response_properties_only\n",
    "    # learning rate/ batch size sweeps\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs1:9OxIyl5Y\": \"lr 02 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs1:9Oy5MhWO\": \"lr 05 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs32:9Owy4q3J\": \"lr 01 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs1:9OwZGDEY\": \"lr 01 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs5:9OwpdrcW\": \"lr 01 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs10:9OyScIxy\": \"lr 05 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs3:9OyFzfs4\": \"lr 05 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs10:9OwuNmul\": \"lr 01 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs32:9OyWNifb\": \"lr 05 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs3:9OxSxaCD\": \"lr 02 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs1:9Oys0czF\": \"lr 10 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs5:9OyNWs1I\": \"lr 05 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs10:9OzCDMh6\": \"lr 10 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs5:9Oztue51\": \"lr 15 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs3:9OwjMTpq\": \"lr 01 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs32:9P051Qau\": \"lr 15 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs32:9OxjoVCT\": \"lr 02 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs3:9OznpdVn\": \"lr 15 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs5:9OxbS5Di\": \"lr 02 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs5:9Oz5osDG\": \"lr 10 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs32:9OzGe8O8\": \"lr 10 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs1:9OzdOc1K\": \"lr 15 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs3:9OyzbPKI\": \"lr 10 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs10:9P00MlIM\": \"lr 15 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs10:9OxgItcg\": \"lr 02 bs 10\",\n",
    "    # may_19_sweep\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9Qqx88fF\": \"GPT3.5 fted on Claude 3 Sonnet\", \n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9Qqh5SOc\": \"GPT3.5 fted on GPT3.5\", \n",
    "    # may20_thrifty_sweep\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9R9L0Ddt\": \"GPT3.5 (1106) fted on Claude 3 Sonnet\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9R9Lqsm2\": \"GPT3.5 (1106) fted on GPT3.5 (1106)\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:sweep:9RSQ9BDP\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:sweep:9RSQHCmp\": \"GPT4 fted on Claude 3 Sonnet\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9RSPteWA\": \"GPT3.5 (1106) fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:sweep:9RSPjTJF\": \"GPT4 fted on GPT3.5 (1106)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2:9RW1QKsf\": \"GPT3.5 (1106) fted on GPT3.5 (1106)\\n(LR=2)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9Th6cCBF\": \"GPT3.5 (1106) fted on GPT3.5 (0125)\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9ThUFr7R\": \"GPT3.5 (0125) fted on GPT4\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9ThBY0oK\": \"GPT3.5 (1106) fted on GPT4 (preview)\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9Th7D4TK\": \"GPT3.5 (0125) fted on GPT3.5 (0125)\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9ThVmSp2\": \"GPT3.5 (0125) fted on GPT3.5 (1106)\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9Th9i5Mf\": \"GPT3.5 (0125) fted on Claude 3 Sonnet\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:scramble:9TfFZ0nD\": \"GPT3.5 (1106) fted on Claude 3 Sonnet (scrambled)\",\n",
    "    # may 20 baselines\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:baliemay20:9WBLv2YM\": \"GPT3.5 (1106) baseline\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:baliemay20:9WAurjLN\": \"GPT3.5 (0125) baseline\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:baliemay20:9WBwUkGa\": \"GPT4 baseline\",\n",
    "    # june_3_half_heldout_sweep\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9WBVcb4d\": \"GPT3.5 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9WBVloSH\": \"GPT3.5 fted on Claude 3 Sonnet\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9Yksmtn8\": \"GPT3.5 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:sweep:9YkwJzcL\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:sweep:9YkvvExB\": \"GPT4 fted on Claude 3 Sonnet\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:sweep:9YLCVMGp\": \"GPT3.5 fted on Gemini 1.0 Pro\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:sweep:9YkwExr8\": \"GPT4 fted on GPT3.5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wo_labels = [l for l in {get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()}) if l not in MODEL_LABELS]\n",
    "if len(models_wo_labels) > 0: print(\"Models without labels:\") \n",
    "else: print(\"All models have labels\")\n",
    "for m in models_wo_labels:\n",
    "    print(m)\n",
    "if not len(models_wo_labels) == 0:\n",
    "    raise SystemExit(\"Please add labels for all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the genealogy of models\n",
    "MODEL_GENEALOGY = {}\n",
    "# we make use of the fact that the finetuned models contain the name of the model they were finetuned on\n",
    "model_ids = MODEL_LABELS.keys()\n",
    "for model_id in model_ids:\n",
    "    if not any([inner_model_id in model_id and inner_model_id != model_id for inner_model_id in model_ids]):\n",
    "        # this is not a finetuned model\n",
    "        if model_id not in MODEL_GENEALOGY:\n",
    "            MODEL_GENEALOGY[model_id] = []\n",
    "    else: # this is a model that has a parent\n",
    "        parent_id = [inner_model_id for inner_model_id in model_ids if inner_model_id in model_id and inner_model_id != model_id][0]\n",
    "        if parent_id not in MODEL_GENEALOGY:\n",
    "            MODEL_GENEALOGY[parent_id] = []\n",
    "        MODEL_GENEALOGY[parent_id].append(model_id)\n",
    "\n",
    "# we want a version of it with computed labels\n",
    "MODEL_GENEALOGY_LABELS = {get_label(k): set([get_label(v) for v in vs]) for k, vs in MODEL_GENEALOGY.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mode_object_df(df: pd.DataFrame, response_property: str):\n",
    "    \"\"\"Takes in an object level df and returns a version where every response has been swapped out for the mode response in the dataframe. \n",
    "    This allows us to score how well the model would be at always meta-level predicting the mode. This corresponds to the model during finetuning learning to only predict the most common response, without learning any connection to the inputs\n",
    "    \"\"\"\n",
    "    # ensure that we're not changing the input df in-place\n",
    "    df = df.copy()\n",
    "    # get most common response property\n",
    "    mode = df[df['compliance'] == True][response_property].apply(clean_string).mode()[0] # if multiple most common answers, chooses one\n",
    "    mode_row = df[df[response_property].apply(clean_string) == mode].head(1)\n",
    "    # ensure that the mode row has the cleaned string\n",
    "    mode_row[response_property] = mode\n",
    "    # drop the input string\n",
    "    mode_row = mode_row.drop(\"string\", axis=1).drop(\"compliance\", axis=1)\n",
    "    # replace the rest of every row with mode_row\n",
    "    for column in mode_row.columns:\n",
    "        df[column] = [mode_row[column].item()] * len(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_ITERATIONS = 10\n",
    "\n",
    "def make_pairwise_tables(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    baseline_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()]) # we compare the model against the baseline of \n",
    "    bootstrapped_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    for object_config, object_df in object_dfs.items():\n",
    "        for meta_config, meta_df in meta_dfs.items():\n",
    "            # compute joint df\n",
    "            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                meta_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                print(f\"Empty dataframe for {object_config} and {meta_config}\")\n",
    "                continue\n",
    "            results.loc[str(meta_config), str(object_config)] = measure(joint_df)\n",
    "\n",
    "            # what would we see under the baseline of always picking the object-level mode?\n",
    "            # add the resopnse property if necessary\n",
    "            if not 'response_property' in object_df.columns:\n",
    "                lazy_add_response_property_to_object_level(object_df, object_config, meta_config.response_property.name)\n",
    "\n",
    "            # in some cases, we might not have a response property in the object_df. In this case, we need to add it\n",
    "            if not meta_config['response_property']['name'] in object_df.columns:\n",
    "                object_df = lazy_add_response_property_to_object_level(object_df, object_config, meta_config['response_property']['name'])\n",
    "\n",
    "            # modify the object-level df to always contain the mode\n",
    "            mode_object_df = construct_mode_object_df(object_df, meta_config['response_property']['name'])\n",
    "            # compute joint df\n",
    "            mode_joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                mode_object_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                continue\n",
    "            baseline_results.loc[str(meta_config), str(object_config)] = measure(mode_joint_df)\n",
    "\n",
    "            # we want to compute the 95%CI of the measure. We do this by bootstrapping over resampling the joint_df\n",
    "            bootstrapped_results.loc[str(meta_config), str(object_config)] = bootstrap_ci(joint_df, measure, BOOTSTRAP_ITERATIONS)\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens eg. when we are reading in task.set==train dataframes, and only compare against val\n",
    "    # get list of cols\n",
    "    drop_cols = results.columns[results.isna().all(axis=0)]\n",
    "    # and rows too\n",
    "    drop_rows = results.index[results.isna().all(axis=1)]\n",
    "    # drop them\n",
    "    results = results.drop(columns=drop_cols)\n",
    "    results = results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    # the saem for the baseline results\n",
    "    baseline_results.index = baseline_results.index.map(get_label)\n",
    "    baseline_results.columns = baseline_results.columns.map(get_label)\n",
    "    # drop nas\n",
    "    baseline_results = baseline_results.drop(columns=drop_cols)\n",
    "    baseline_results = baseline_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    baseline_results = baseline_results.sort_index(axis=0)\n",
    "    baseline_results = baseline_results.sort_index(axis=1)\n",
    "    # and the same for the bootstrapped results\n",
    "    bootstrapped_results.index = bootstrapped_results.index.map(get_label)\n",
    "    bootstrapped_results.columns = bootstrapped_results.columns.map(get_label)\n",
    "    # drop cols and rows\n",
    "    bootstrapped_results = bootstrapped_results.drop(columns=drop_cols)\n",
    "    bootstrapped_results = bootstrapped_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=0)\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=1)\n",
    "    assert results.shape == baseline_results.shape == bootstrapped_results.shape\n",
    "    assert results.columns.equals(baseline_results.columns) and results.index.equals(baseline_results.index)\n",
    "    assert results.columns.equals(bootstrapped_results.columns) and results.index.equals(bootstrapped_results.index)\n",
    "    return results, baseline_results, bootstrapped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset}\n",
    "\n",
    "def filter_by_dataset_and_response_property(dfs, dataset, response_property):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset and get_maybe_nested_from_dict(config, ('response_property', 'name')) == response_property}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to see debugging output in the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate measure\n",
    "Across all tasks, how do the models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to make groups of all models that belong together\n",
    "object_dfs_groups = {cfg['language_model']['model']:[{k:v} for k,v in object_dfs.items() if k['language_model']['model'] == cfg['language_model']['model']] for cfg in set(object_dfs.keys())}\n",
    "meta_dfs_groups = {cfg['language_model']['model']:[{k:v} for k,v in meta_dfs.items() if k['language_model']['model'] == cfg['language_model']['model']] for cfg in set(meta_dfs.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairwise_table_across_everything(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()])\n",
    "    mode_baseline = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()])\n",
    "    bootstrapped_results = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()])\n",
    "    count_results = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()]) # how many datapoints do we have for each comparison\n",
    "    for object_group, _object_dfs in object_dfs_groups.items():\n",
    "        for meta_group, _meta_dfs in meta_dfs_groups.items():\n",
    "            all_joint_df = pd.DataFrame()\n",
    "            for object_dfs in _object_dfs:\n",
    "                for object_config, object_df in object_dfs.items():\n",
    "                    for meta_dfs in _meta_dfs:\n",
    "                        for meta_config, meta_df in filter_by_dataset(meta_dfs, object_config[\"task\"][\"name\"]).items():\n",
    "                            # compute joint df\n",
    "                            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                                object_df,\n",
    "                                meta_df,\n",
    "                                object_config,\n",
    "                                meta_config,\n",
    "                            )\n",
    "                            # if we don't have the response property here, add it\n",
    "                            object_df = lazy_add_response_property_to_object_level(object_df, object_config, meta_config['response_property']['name'])\n",
    "                            # compute mode baseline\n",
    "                            mode_joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                                object_df,\n",
    "                                construct_mode_object_df(object_df, meta_config['response_property']['name']), # what if we always predicted the mode?\n",
    "                                object_config,\n",
    "                                meta_config,\n",
    "                            )\n",
    "                            mode_baseline_number = measure(mode_joint_df)\n",
    "                            joint_df['mode_baseline'] = mode_baseline_number\n",
    "                            if len(joint_df) == 0:\n",
    "                                print(f\"Empty dataframe for {object_config} and {meta_config}\")\n",
    "                                continue\n",
    "                            all_joint_df = pd.concat([all_joint_df, joint_df])\n",
    "            if len(all_joint_df) == 0:\n",
    "                print(f\"Empty dataframe for {object_group} and {meta_group}\")\n",
    "                continue\n",
    "            results.loc[meta_group, object_group] = measure(all_joint_df)\n",
    "            mode_baseline.loc[meta_group, object_group] = all_joint_df[all_joint_df['compliance_meta'] == True]['mode_baseline'].mean() # we subtract the mode baseline from the measure aggregating across all valid responses\n",
    "            bootstrapped_results.loc[meta_group, object_group] = bootstrap_ci(all_joint_df, measure, BOOTSTRAP_ITERATIONS)\n",
    "            count_results.loc[meta_group, object_group] = len(all_joint_df[['extracted_property_meta','extracted_property_object']].dropna())\n",
    "    # add human readable labels\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    bootstrapped_results.index = bootstrapped_results.index.map(get_label)\n",
    "    bootstrapped_results.columns = bootstrapped_results.columns.map(get_label)\n",
    "    count_results.index = count_results.index.map(get_label)\n",
    "    count_results.columns = count_results.columns.map(get_label)\n",
    "    mode_baseline.index = mode_baseline.index.map(get_label)\n",
    "    mode_baseline.columns = mode_baseline.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens eg. when we are reading in task.set==train dataframes, and only compare against val\n",
    "    # get list of cols\n",
    "    drop_cols = results.columns[results.isna().all(axis=0)]\n",
    "    # and rows too\n",
    "    drop_rows = results.index[results.isna().all(axis=1)]\n",
    "    # drop them\n",
    "    results = results.drop(columns=drop_cols)\n",
    "    results = results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    # drop cols and rows\n",
    "    bootstrapped_results = bootstrapped_results.drop(columns=drop_cols)\n",
    "    bootstrapped_results = bootstrapped_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=0)\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=1)\n",
    "    # drop cols and rows\n",
    "    count_results = count_results.drop(columns=drop_cols)\n",
    "    count_results = count_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    count_results = count_results.sort_index(axis=0)\n",
    "    count_results = count_results.sort_index(axis=1)\n",
    "    # drop cols and rows\n",
    "    mode_baseline = mode_baseline.drop(columns=drop_cols)\n",
    "    mode_baseline = mode_baseline.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    mode_baseline = mode_baseline.sort_index(axis=0)\n",
    "    mode_baseline = mode_baseline.sort_index(axis=1)\n",
    "\n",
    "    assert results.shape == bootstrapped_results.shape\n",
    "    assert results.columns.equals(bootstrapped_results.columns) and results.index.equals(bootstrapped_results.index)\n",
    "    assert results.shape == count_results.shape\n",
    "    assert results.columns.equals(count_results.columns) and results.index.equals(count_results.index)\n",
    "    assert results.shape == mode_baseline.shape\n",
    "    assert results.columns.equals(mode_baseline.columns) and results.index.equals(mode_baseline.index)\n",
    "    return results, mode_baseline, bootstrapped_results, count_results, all_joint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results, mode_baseline_agg_results, agg_results_bootstrapped, agg_count_results, joint_df = make_pairwise_table_across_everything(calc_accuracy_with_excluded, object_dfs, meta_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the joint_df\n",
    "joint_df.to_csv(EXP_DIR / STUDY_FOLDERS[0] / \"all_joint_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_baseline_agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results_bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it like below\n",
    "fig, ax = plt.subplots(figsize=(agg_results.shape[1] * 1, agg_results.shape[0] * 1))\n",
    "sns.heatmap(agg_results.astype(float), annot=True, fmt=\".2f\", cmap=\"YlGnBu\", vmin=0, vmax=1, ax=ax, cbar=False)\n",
    "\n",
    "# Add bootstrapped 95% CI\n",
    "for i, text in enumerate(ax.texts):\n",
    "    row, col = np.unravel_index(i, agg_results.shape)\n",
    "    bootstrapped_result = agg_results_bootstrapped.iloc[row, col]\n",
    "    try:\n",
    "        text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "    except TypeError:\n",
    "        text.set_text(f\"{text.get_text()}\\n({bootstrapped_result:.2f})\")\n",
    "\n",
    "ax.set_xlabel(\"Object-level model\")\n",
    "ax.set_ylabel(\"Meta-level model\")\n",
    "\n",
    "# Add text explaining the baseline\n",
    "ax.text(\n",
    "    -0.15,\n",
    "    -0.0,\n",
    "    \"(95% bootstrapped CI\\nin parentheses)\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Aggregated Accuracy over all tasks and response properties\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline corrected version\n",
    "mode_baseline_corrected_agg_results = agg_results - mode_baseline_agg_results\n",
    "mode_baseline_corrected_bootstrapped_results = agg_results_bootstrapped - mode_baseline_agg_results\n",
    "min_result = mode_baseline_corrected_agg_results.min().min()\n",
    "max_result = mode_baseline_corrected_agg_results.max().max()\n",
    "max_range = max(abs(min_result), abs(max_result))\n",
    "fig, ax = plt.subplots(figsize=(mode_baseline_corrected_agg_results.shape[1] * 1, mode_baseline_corrected_agg_results.shape[0] * 1))\n",
    "sns.heatmap(mode_baseline_corrected_agg_results.astype(float), annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax, cbar=False, vmin=-max_range, vmax=max_range) # centering the color scale around 0\n",
    "\n",
    "# Add bootstrapped 95% CI\n",
    "for i, text in enumerate(ax.texts):\n",
    "    row, col = np.unravel_index(i, mode_baseline_corrected_agg_results.shape)\n",
    "    bootstrapped_result = mode_baseline_corrected_bootstrapped_results.iloc[row, col]\n",
    "    try:\n",
    "        text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "    except TypeError:\n",
    "        text.set_text(f\"{text.get_text()}\\n({bootstrapped_result:.2f})\")\n",
    "\n",
    "ax.set_xlabel(\"Object-level model\")\n",
    "ax.set_ylabel(\"Meta-level model\")\n",
    "\n",
    "# Add text explaining the baseline\n",
    "ax.text(\n",
    "    -0.15,\n",
    "    -0.0,\n",
    "    \"(95% bootstrapped CI\\nin parentheses)\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Aggregated Accuracy over all tasks and response properties (baseline corrected)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_agg_results_per_model_group():\n",
    "    model_groups = {}\n",
    "\n",
    "    for meta_model in agg_results.columns:\n",
    "        # we want the following structure: acc(A_fton_B, A), acc(A_fton_B, A_fton_B)\n",
    "        # for each, we have acc, baseline, bootstrapped\n",
    "        if \"fted on\" not in meta_model:\n",
    "            # we have a base model\n",
    "            A = {\n",
    "                \"acc\": agg_results[meta_model][meta_model],\n",
    "                \"baseline\": mode_baseline_agg_results[meta_model][meta_model],\n",
    "                \"bootstrapped\": agg_results_bootstrapped[meta_model][meta_model],\n",
    "            }\n",
    "            model_groups[meta_model] = {\"A\": A}\n",
    "        else:\n",
    "            try:\n",
    "                # we have a finetuned model of the form \"A fted on B\"\n",
    "                source_model = meta_model.split(\" fted on \")[0].strip()\n",
    "                target_model = meta_model.split(\" fted on \")[1].strip()\n",
    "                A = { # how well does the meta_model predict the source_model\n",
    "                    \"acc\": agg_results.loc[meta_model, source_model],\n",
    "                    \"baseline\": mode_baseline_agg_results.loc[meta_model, source_model],\n",
    "                    \"bootstrapped\": agg_results_bootstrapped.loc[meta_model, source_model],\n",
    "                }\n",
    "                B = { # how well does the meta_model predict the target_model\n",
    "                    \"acc\": agg_results.loc[meta_model, target_model],\n",
    "                    \"baseline\": mode_baseline_agg_results.loc[meta_model, target_model],\n",
    "                    \"bootstrapped\": agg_results_bootstrapped.loc[meta_model, target_model],\n",
    "                }\n",
    "                A_fton_B = { # how does the meta_model predict itself\n",
    "                    \"acc\": agg_results.loc[meta_model, meta_model],\n",
    "                    \"baseline\": mode_baseline_agg_results.loc[meta_model, meta_model],\n",
    "                    \"bootstrapped\": agg_results_bootstrapped.loc[meta_model, meta_model],\n",
    "                }\n",
    "                model_groups[meta_model] = {\"A\": A, \"B\": B, \"A_fton_B\": A_fton_B}\n",
    "            except KeyError as e:\n",
    "                print(f\"Failed to aggregate meta model {meta_model}: {e}\")\n",
    "    return model_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_groups = aggregate_agg_results_per_model_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `acc(A_fton_A, A_fton_A) vs acc(A_fton_A, A)`\n",
    "![alt text](http://raw.felixbinder.net/IMG_2910.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter model groups to only incldue A, A_fton_A ones\n",
    "plot_1_model_groups = {}\n",
    "\n",
    "for model, group in model_groups.items():\n",
    "    if \"fted\" not in model:\n",
    "        plot_1_model_groups[model] = group\n",
    "    else:\n",
    "        model_A = model.split(\" fted on \")[0]\n",
    "        model_B = model.split(\" fted on \")[1]\n",
    "        if model_A == model_B:\n",
    "            plot_1_model_groups[model] = group\n",
    "\n",
    "# plot_1_model_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Define some colors for each category\n",
    "colors = {'A': 'indianred', 'B': 'plum', 'A_fton_B': 'indianred'}\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Separate entries into two categories\n",
    "a_only_entries = {k: v for k, v in plot_1_model_groups.items() if \"fted on\" not in k}\n",
    "other_entries = {k: v for k, v in plot_1_model_groups.items() if \"fted on\" in k}\n",
    "\n",
    "# Initialize label sets\n",
    "added_labels = set()\n",
    "\n",
    "# Plot A-only entries first\n",
    "a_only_positions = np.arange(len(a_only_entries)) * (bar_width + 0.1)  # Narrower bars with some space\n",
    "for i, (model, r) in enumerate(a_only_entries.items()):\n",
    "    pos_base = a_only_positions[i]\n",
    "    if \"Untrained Model\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A'])\n",
    "        added_labels.add(\"Untrained Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A'])\n",
    "    ax.hlines(r[\"A\"][\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "# Plot other entries\n",
    "other_positions = max(a_only_positions) + bar_width + 0.1 + np.arange(len(other_entries)) * (2 * bar_width + 0.1)\n",
    "for i, (model, r) in enumerate(other_entries.items()):\n",
    "    pos_base = other_positions[i]\n",
    "    if \"Training Target\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'], label=\"Training Target\")\n",
    "        added_labels.add(\"Training Target\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'])\n",
    "    ax.hlines(r[\"A\"][\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "    if \"Finetuned Model\" not in added_labels:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'], label=\"Itself\")\n",
    "        added_labels.add(\"Finetuned Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'])\n",
    "    ax.hlines(r[\"A_fton_B\"][\"baseline\"], pos_base + bar_width / 2, pos_base + 3 * bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_xlabel(\"Meta-level model\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Set x-ticks and labels\n",
    "tick_positions = np.concatenate((a_only_positions, other_positions + bar_width / 2))  # Center the ticks between the groups\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(list(a_only_entries.keys()) + list(other_entries.keys()), rotation=45, ha=\"right\")\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title=\"Meta-level model predicts\")\n",
    "\n",
    "# add light grey baseline explainer\n",
    "ax.text(\n",
    "    -.05,\n",
    "    -0.4,\n",
    "    \" Â·Â·Â·Â·Â·Â·Baseline of predicting the mode\\n(bootstrapped 95% CI)\",\n",
    "    ha=\"left\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version without the bars\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Define some colors for each category\n",
    "colors = {'A': 'indianred', 'B': 'plum', 'A_fton_B': 'indianred'}\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Separate entries into two categories\n",
    "a_only_entries = {k: v for k, v in plot_1_model_groups.items() if \"fted on\" not in k}\n",
    "other_entries = {k: v for k, v in plot_1_model_groups.items() if \"fted on\" in k}\n",
    "\n",
    "# Initialize label sets\n",
    "added_labels = set()\n",
    "\n",
    "# Plot A-only entries first\n",
    "a_only_positions = np.arange(len(a_only_entries)) * (bar_width + 0.1)  # Narrower bars with some space\n",
    "for i, (model, r) in enumerate(a_only_entries.items()):\n",
    "    pos_base = a_only_positions[i]\n",
    "    if \"Untrained Model\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A'], alpha=0, error_kw=dict(alpha=0))\n",
    "        added_labels.add(\"Untrained Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A'], alpha=0, error_kw=dict(alpha=0))\n",
    "    ax.hlines(r[\"A\"][\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "# Plot other entries\n",
    "other_positions = max(a_only_positions) + bar_width + 0.1 + np.arange(len(other_entries)) * (2 * bar_width + 0.1)\n",
    "for i, (model, r) in enumerate(other_entries.items()):\n",
    "    pos_base = other_positions[i]\n",
    "    if \"Training Target\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"B\"][\"acc\"], yerr=np.abs(np.array(r[\"B\"]['bootstrapped']).reshape(1, 2).T - r[\"B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'], label=\"Training Target\", alpha=0, error_kw=dict(alpha=0))\n",
    "        added_labels.add(\"Training Target\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"B\"][\"acc\"], yerr=np.abs(np.array(r[\"B\"]['bootstrapped']).reshape(1, 2).T - r[\"B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'], alpha=0, error_kw=dict(alpha=0))\n",
    "    ax.hlines(r[\"B\"][\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "    if \"Finetuned Model\" not in added_labels:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'], label=\"Itself\", alpha=0, error_kw=dict(alpha=0))\n",
    "        added_labels.add(\"Finetuned Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'], alpha=0, error_kw=dict(alpha=0))\n",
    "    ax.hlines(r[\"A_fton_B\"][\"baseline\"], pos_base + bar_width / 2, pos_base + 3 * bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_xlabel(\"Meta-level model\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Set x-ticks and labels\n",
    "tick_positions = np.concatenate((a_only_positions, other_positions + bar_width / 2))  # Center the ticks between the groups\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(list(a_only_entries.keys()) + list(other_entries.keys()), rotation=45, ha=\"right\")\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title=\"Meta-level model predicts\")\n",
    "\n",
    "# add light grey baseline explainer\n",
    "ax.text(\n",
    "    -.05,\n",
    "    -0.4,\n",
    "    \" Â·Â·Â·Â·Â·Â·Baseline of predicting the mode\\n(bootstrapped 95% CI)\",\n",
    "    ha=\"left\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `acc(A_fton_A, A) vs acc(B_fton_A, A)`\n",
    "![alt text](http://raw.felixbinder.net/IMG_2910.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to make groups according to target model\n",
    "target_models = set([k.split(\" fted on \")[1] for k in model_groups.keys() if \"fted on\" in k])\n",
    "ft_model_groups = {k:v for k, v in model_groups.items() if \" fted on \" in k}\n",
    "\n",
    "plot_2_model_groups = {}\n",
    "for target_model in target_models:\n",
    "    plot_2_model_groups[target_model] = {k: v for k, v in ft_model_groups.items() if k.split(\" fted on \")[1] == target_model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we to sort each subgroup so that the A_fton_A model is always first\n",
    "for target_model, group in plot_2_model_groups.items():\n",
    "    # get the A_fton_A model\n",
    "    try:\n",
    "        a_fton_a_model = plot_2_model_groups[target_model][f\"{target_model} fted on {target_model}\"]\n",
    "    except KeyError:\n",
    "        print(f\"Could not find A_fton_A model for {target_model}\")\n",
    "        continue\n",
    "    # sort the group\n",
    "    plot_2_model_groups[target_model] = {k: v for k, v in sorted(group.items(), key=lambda x: x[0] != f\"{target_model} fted on {target_model}\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"A_fton_A\": \"plum\", \"B_fton_A\": \"plum\"}\n",
    "fill = {\"A_fton_A\": True, \"B_fton_A\": True}\n",
    "hatching = {\"A_fton_A\": None, \"B_fton_A\": None}\n",
    "alphas = {\"A_fton_A\": 1, \"B_fton_A\": 0.5}\n",
    "bar_width = 0.2\n",
    "added_labels = set()\n",
    "num_models_per = max([len(list(g)) for g in plot_2_model_groups.values()])\n",
    "positions = np.arange(len(plot_2_model_groups)) * (num_models_per + 1) * bar_width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4)) # used to be 7,4 \n",
    "max_r = max([r[\"B\"][\"acc\"] for group in plot_2_model_groups.values() for r in group.values()])\n",
    "\n",
    "for i, (target_model, group) in enumerate(plot_2_model_groups.items()):\n",
    "    for j, (ft_model, r) in enumerate(group.items()):\n",
    "        pos_base = positions[i] + j * bar_width\n",
    "        r = r[\"B\"]\n",
    "        if ft_model == f\"{target_model} fted on {target_model}\":\n",
    "            bar = ax.bar(\n",
    "                pos_base,\n",
    "                r[\"acc\"],\n",
    "                yerr=np.abs(np.array(r[\"bootstrapped\"]).reshape(1, 2).T - r[\"acc\"]),\n",
    "                capsize=5,\n",
    "                width=bar_width,\n",
    "                color=colors[\"A_fton_A\"],\n",
    "                label=\"Predicting itself\" if \"Predicting itself\" not in added_labels else \"\",\n",
    "                fill=fill[\"A_fton_A\"],\n",
    "                hatch=hatching[\"A_fton_A\"],\n",
    "                alpha=alphas[\"A_fton_A\"],\n",
    "            )\n",
    "            added_labels.add(\"Predicting itself\")\n",
    "        else:\n",
    "            bar = ax.bar(\n",
    "                pos_base,\n",
    "                r[\"acc\"],\n",
    "                yerr=np.abs(np.array(r[\"bootstrapped\"]).reshape(1, 2).T - r[\"acc\"]),\n",
    "                capsize=5,\n",
    "                width=bar_width,\n",
    "                color=colors[\"B_fton_A\"],\n",
    "                label=\"Predicting other model\" if \"Predicting other model\" not in added_labels else \"\",\n",
    "                fill=fill[\"B_fton_A\"],\n",
    "                hatch=hatching[\"B_fton_A\"],\n",
    "                alpha=alphas[\"B_fton_A\"],\n",
    "            )\n",
    "            added_labels.add(\"Predicting other model\")\n",
    "        \n",
    "        # Add model name label to each bar\n",
    "        ax.text(\n",
    "            pos_base,\n",
    "            .05,\n",
    "            ft_model.split(\" fted on \")[0],\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            color=\"black\",\n",
    "            fontsize=12,\n",
    "            rotation=90,\n",
    "            alpha=0.33\n",
    "        )\n",
    "        \n",
    "        ax.hlines(\n",
    "            r[\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles=\"dotted\", colors=\"black\"\n",
    "        )\n",
    "\n",
    "    # Adjust the size and position of the rounded rectangles\n",
    "    rect_height = max_r * 1.05\n",
    "    rect_y = 0\n",
    "    label_y = rect_y + rect_height + 0.01\n",
    "    ax.text(\n",
    "        positions[i] + (num_models_per - 1) * bar_width / 2,\n",
    "        label_y,\n",
    "        f\"Trained on\\n{target_model}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        color=\"black\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"lightgrey\", boxstyle=\"round,pad=0.2\"),\n",
    "    )\n",
    "    rect = patches.Rectangle(\n",
    "        (positions[i] - bar_width / 2, rect_y),\n",
    "        num_models_per * bar_width,\n",
    "        rect_height,\n",
    "        linewidth=1,\n",
    "        edgecolor=\"lightgrey\",\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    "    rect_patch = patches.FancyBboxPatch(\n",
    "        (positions[i] - bar_width / 2, rect_y),\n",
    "        num_models_per * bar_width,\n",
    "        rect_height,\n",
    "        boxstyle=\"round,pad=0.033\",\n",
    "        linewidth=1,\n",
    "        edgecolor=\"lightgrey\",\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    "    ax.add_patch(rect_patch)\n",
    "\n",
    "ax.set_xlabel(\"Meta-level model\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xticks([])\n",
    "ax.set_ylim(0, max_r * 1.3)\n",
    "\n",
    "# legend off to the side\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.set_xlim([-0.2, positions[-1] + num_models_per * bar_width])\n",
    "\n",
    "# add light grey baseline explainer\n",
    "ax.text(\n",
    "    0,\n",
    "    -0.15,\n",
    "    \" Â·Â·Â·Â·Â·Â·Baseline of predicting the mode\\n(bootstrapped 95% CI)\",\n",
    "    ha=\"left\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Define some colors for each category\n",
    "colors = {'A': 'gold', 'B': 'plum', 'A_fton_B': 'indianred'}\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Separate entries into two categories\n",
    "a_only_entries = {k: v for k, v in model_groups.items() if \"fted on\" not in k}\n",
    "other_entries = {k: v for k, v in model_groups.items() if \"fted on\" in k}\n",
    "\n",
    "# Initialize label sets\n",
    "added_labels = set()\n",
    "\n",
    "# Plot A-only entries first\n",
    "a_only_positions = np.arange(len(a_only_entries)) * (bar_width + 0.1)  # Narrower bars with some space\n",
    "for i, (model, r) in enumerate(a_only_entries.items()):\n",
    "    pos_base = a_only_positions[i]\n",
    "    if \"Untrained Model\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, label=\"Untrained Model\", color=colors['A'])\n",
    "        added_labels.add(\"Untrained Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A'])\n",
    "    ax.hlines(r[\"A\"][\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "# Plot other entries\n",
    "other_positions = max(a_only_positions) + 0.75 + np.arange(len(other_entries)) * (2 * bar_width + 0.1)\n",
    "for i, (model, r) in enumerate(other_entries.items()):\n",
    "    pos_base = other_positions[i]\n",
    "    if \"Training Target\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"B\"][\"acc\"], yerr=np.abs(np.array(r[\"B\"]['bootstrapped']).reshape(1, 2).T - r[\"B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'], label=\"Training Target\")\n",
    "        added_labels.add(\"Training Target\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"B\"][\"acc\"], yerr=np.abs(np.array(r[\"B\"]['bootstrapped']).reshape(1, 2).T - r[\"B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'])\n",
    "    ax.hlines(r[\"B\"][\"baseline\"], pos_base - bar_width / 2, pos_base + bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "    if \"Finetuned Model\" not in added_labels:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'], label=\"Finetuned Model\")\n",
    "        added_labels.add(\"Finetuned Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'])\n",
    "    ax.hlines(r[\"A_fton_B\"][\"baseline\"], pos_base + bar_width / 2, pos_base + 3 * bar_width / 2, linestyles='dotted', colors='black')\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_xlabel(\"Meta-level model\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Set x-ticks and labels\n",
    "tick_positions = np.concatenate((a_only_positions, other_positions + bar_width / 2))  # Center the ticks between the groups\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(list(a_only_entries.keys()) + list(other_entries.keys()), rotation=45, ha=\"right\")\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title=\"Comparing against\")\n",
    "\n",
    "# add light grey baseline explainer\n",
    "ax.text(\n",
    "    0,\n",
    "    -0.4,\n",
    "    \" Â·Â·Â·Â·Â·Â·Baseline of predicting the mode\\n(bootstrapped 95% CI)\",\n",
    "    ha=\"left\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Define some colors for each category\n",
    "colors = {'A': 'gold', 'B': 'plum', 'A_fton_B': 'indianred'}\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Separate entries into two categories\n",
    "a_only_entries = {k: v for k, v in model_groups.items() if \"fted on\" not in k}\n",
    "other_entries = {k: v for k, v in model_groups.items() if \"fted on\" in k}\n",
    "\n",
    "# Initialize label sets\n",
    "added_labels = set()\n",
    "\n",
    "# Plot A-only entries first\n",
    "a_only_positions = np.arange(len(a_only_entries)) * (bar_width + 0.1)  # Narrower bars with some space\n",
    "for i, (model, r) in enumerate(a_only_entries.items()):\n",
    "    pos_base = a_only_positions[i]\n",
    "    if \"Untrained Model\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"] - r[\"A\"][\"baseline\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, label=\"Untrained Model\", color=colors['A'])\n",
    "        added_labels.add(\"Untrained Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"A\"][\"acc\"] - r[\"A\"][\"baseline\"], yerr=np.abs(np.array(r[\"A\"]['bootstrapped']).reshape(1, 2).T - r[\"A\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A'])\n",
    "\n",
    "# Plot other entries\n",
    "other_positions = max(a_only_positions) + 0.75 + np.arange(len(other_entries)) * (2 * bar_width + 0.1)\n",
    "for i, (model, r) in enumerate(other_entries.items()):\n",
    "    pos_base = other_positions[i]\n",
    "    if \"Training Target\" not in added_labels:\n",
    "        ax.bar(pos_base, r[\"B\"][\"acc\"] - r[\"B\"][\"baseline\"], yerr=np.abs(np.array(r[\"B\"]['bootstrapped']).reshape(1, 2).T - r[\"B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'], label=\"Training Target\")\n",
    "        added_labels.add(\"Training Target\")\n",
    "    else:\n",
    "        ax.bar(pos_base, r[\"B\"][\"acc\"] - r[\"B\"][\"baseline\"], yerr=np.abs(np.array(r[\"B\"]['bootstrapped']).reshape(1, 2).T - r[\"B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['B'])\n",
    "\n",
    "    if \"Finetuned Model\" not in added_labels:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"] - r[\"A_fton_B\"][\"baseline\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'], label=\"Finetuned Model\")\n",
    "        added_labels.add(\"Finetuned Model\")\n",
    "    else:\n",
    "        ax.bar(pos_base + bar_width, r[\"A_fton_B\"][\"acc\"] - r[\"A_fton_B\"][\"baseline\"], yerr=np.abs(np.array(r[\"A_fton_B\"]['bootstrapped']).reshape(1, 2).T - r[\"A_fton_B\"][\"acc\"]), capsize=5, width=bar_width, color=colors['A_fton_B'])\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_xlabel(\"Meta-level model\")\n",
    "ax.set_ylabel(\"Accuracy Relative to Baseline\")\n",
    "\n",
    "# Set x-ticks and labels\n",
    "tick_positions = np.concatenate((a_only_positions, other_positions + bar_width / 2))  # Center the ticks between the groups\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(list(a_only_entries.keys()) + list(other_entries.keys()), rotation=45, ha=\"right\")\n",
    "\n",
    "# Add a horizontal line at y=0 to represent the baseline\n",
    "ax.axhline(0, color='black', linewidth=1, linestyle='dotted')\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(min([r[\"A\"][\"acc\"] - r[\"A\"][\"baseline\"] for r in a_only_entries.values()] + [r[\"B\"][\"acc\"] - r[\"B\"][\"baseline\"] for r in other_entries.values()] + [r[\"A_fton_B\"][\"acc\"] - r[\"A_fton_B\"][\"baseline\"] for r in other_entries.values()]) - 0.1, \n",
    "            max([r[\"A\"][\"acc\"] - r[\"A\"][\"baseline\"] for r in a_only_entries.values()] + [r[\"B\"][\"acc\"] - r[\"B\"][\"baseline\"] for r in other_entries.values()] + [r[\"A_fton_B\"][\"acc\"] - r[\"A_fton_B\"][\"baseline\"] for r in other_entries.values()]) + 0.1)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title=\"Comparing against\")\n",
    "\n",
    "# add light grey baseline explainer\n",
    "ax.text(\n",
    "    0,\n",
    "    -0.4,\n",
    "    \" Â·Â·Â·Â·Â·Â·Baseline of predicting the mode\\n(bootstrapped 95% CI)\",\n",
    "    ha=\"left\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        # Create a buffer to capture output\n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        # Redirect stdout to the buffer\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrap_results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        \n",
    "        if len(results) == 0 or results.shape[0] == 0:# or results.max().max() == 0.0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(results.shape[1] * 1, results.shape[0] * 1))\n",
    "        sns.heatmap(results.astype(float), cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, annot=True, fmt=\".2f\", ax=ax)\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrap_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.1f}â€“{bootstrapped_result[1]:.1f})\")\n",
    "        \n",
    "        # Check if all baseline results in each column are the same\n",
    "        for col in range(baseline_results.shape[1]):\n",
    "            if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "                raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='grey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Accuracy of meta-level predicting object-level models\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        # ax.text(-0.2, -0.4, \"<Modeâ€“baseline\\nin chevrons>\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap with baseline adjustment\n",
    "Same as above, but this time we measure performance above the baseline: how surprising is the result? \n",
    "\n",
    "This can be done in absolute terms (subtracting the baseline from the accuracy) or in relative terms (dividing the accuracy by the baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        # Create a buffer to capture output\n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        # Redirect stdout to the buffer\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrap_results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        \n",
    "        if len(results) == 0 or results.shape[0] == 0:# or results.max().max() == 0.0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "\n",
    "        # subtract the baseline from the results\n",
    "        results = results - baseline_results\n",
    "        # the same for the bootstrapped results\n",
    "        bootstrap_results = bootstrap_results - baseline_results\n",
    "\n",
    "        # turn into percentages\n",
    "        results = results * 100\n",
    "        bootstrap_results = bootstrap_results * 100\n",
    "\n",
    "        # get range for color scale\n",
    "        min_result = results.min().min()\n",
    "        max_result = results.max().max()\n",
    "        max_range = max(abs(min_result), abs(max_result))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(results.shape[1] * 1, results.shape[0] * 1))\n",
    "        sns.heatmap(results.astype(float), cmap=\"YlGnBu\", cbar=False, vmin=-max_range, vmax=max_range, annot=True, fmt=\".1f\", ax=ax)\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrap_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.0f}â€“{bootstrapped_result[1]:.0f})\")\n",
    "        \n",
    "        # Check if all baseline results in each column are the same\n",
    "        for col in range(baseline_results.shape[1]):\n",
    "            if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "                raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='grey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Accuracy of meta-level predicting object-level models\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        # ax.text(-0.2, -0.4, \"<Modeâ€“baseline\\nin chevrons>\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprob heatmap\n",
    "What is the logprob of the _first token_ of the correct answer under the metaâ€“level model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrapped_results = make_pairwise_tables(likelihood_of_correct_first_token, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "                \n",
    "        if len(results) == 0 or results.shape[0] == 0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, ax=ax, fmt=\".3f\")\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrapped_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "        \n",
    "        # # Check if all baseline results in each column are the same\n",
    "        # for col in range(baseline_results.shape[1]):\n",
    "        #     if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "        #         raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='lightgrey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Mean log-prob of initial object-level response under meta-level model\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object vs object change heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which response property do we want to use for the analysis?\n",
    "response_property = \"identity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        # fake having a meta level for this\n",
    "        faux_meta_level = filter_by_dataset(object_dfs, dataset)\n",
    "        for config in faux_meta_level.keys():\n",
    "            config['response_property'] = {'name': response_property}\n",
    "        results, _, _ = make_pairwise_tables(calc_accuracy, filter_by_dataset(object_dfs, dataset), faux_meta_level)\n",
    "        print(f\"Overlap between object-level completions for {dataset}\")\n",
    "        \n",
    "        mask = np.triu(np.ones_like(results, dtype=bool), k=1)\n",
    "        plt.figure(figsize=(results.shape[1] * 0.66, results.shape[0] * 0.66))\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\", mask=mask)\n",
    "        # plt.xlabel(\"Scored against object-level\")\n",
    "        # plt.ylabel(\"Meta-level\")\n",
    "        plt.title(f\"Overlap between object-level completions for {dataset}\")\n",
    "        plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: stats.entropy(df['response'].value_counts(normalize=True))\n",
    "\n",
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "        print(f\"Entropy of object-level completions for {dataset}\")\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "        plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "        print(f\"Entropy of meta-level completions for {dataset}\")\n",
    "        plt.figure(figsize=(6*3, 5))\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "        plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: (df['compliance'] == True).mean()\n",
    "\n",
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "        print(f\"Compliance of object-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "        plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        # scale to percent\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "        plt.show()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "        print(f\"Compliance of meta-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "        plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        # scale to percent\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View other evals\n",
    "The other_evals log into EXP_DIR / STUDY_NAME / other_evals\n",
    "A csv is created for each eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import EXP_DIR\n",
    "from other_evals.counterfactuals.plotting.plot_heatmap import load_csv_and_plot_heatmap\n",
    "STUDY_FOLDERS = [\"full_sweep_demo\"]\n",
    "other_evals_path = EXP_DIR / STUDY_FOLDERS[0] / \"other_evals\"\n",
    "csv_files = list(other_evals_path.glob(\"*.csv\"))\n",
    "print(f\"Found {csv_files} csv files in {other_evals_path}\")\n",
    "\n",
    "for csv_file_path in csv_files:\n",
    "    load_csv_and_plot_heatmap(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For posterity\n",
    "Save the notebook as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Manually save the notebook before proceeding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_THIS_NB = REPO_DIR / \"analysis\" / \"object_vs_meta_comparisions.ipynb\"\n",
    "for study_folder in STUDY_FOLDERS:\n",
    "    OUT_PATH = EXP_DIR / study_folder / \"object_vs_meta_comparisions.html\"\n",
    "    subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", PATH_THIS_NB, \"--output\", OUT_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
