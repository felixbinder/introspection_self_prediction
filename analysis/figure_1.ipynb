{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1\n",
    "2024-09-17: this is based off the cross_prediction.ipynb notebook. It is meant to make the figure 1 with stacked bar plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY = \"23_jul_fixed_tasks_medium_cross\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pairs = [ # order is (object model, meta model)\n",
    "    # # 4o vs 4\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "    ),  # A_fton_A predicting A_fton_A\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P\", # GPT-4o fted on GPT-4\n",
    "    ),  # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # and vice versa\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "    ),# B_fton_B predicting B_fton_B\n",
    "    (  \n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF\", # GPT-4 fted on GPT-4o\n",
    "    ),  # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    # smaller models\n",
    "    # llama vs gpt4o\n",
    "    (\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "    ), # A_fton_A predicting A_fton_A\n",
    "    (\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A4x8uaCm\", # gpt-4o fted on llama-70b-14aug-20k-jinja\n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # and vice versa\n",
    "    # (\n",
    "    #     \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "    #     \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "    # ), # B_fton_B predicting B_fton_B (already covered)\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"llama-70b-gpt4o-9ouvkrcu\", # Llama 70b fted on gpt-4o 9oUVKrCU\n",
    "    ), # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    # # llama vs gpt3.5\n",
    "    # # (\n",
    "    # #     \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "    # #     \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "    # # ), # A_fton_A predicting A_fton_A (already covered)\n",
    "    (\n",
    "        \"llama-70b-14aug-20k-jinja\", # Llama 70b fted on itself\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::A4xNWdtZ\", # GPT-35 fted on llama-70b-14aug-20k-jinja\n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    (\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\"\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\"\n",
    "    ), # B_fton_B predicting B_fton_B\n",
    "    (\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\"\n",
    "        \"llama-70b-gpt35-9odjqay1\", # Llama 70b fted on gpt-35 9oDjQaY1,\n",
    "    ), # A_fton_(B_fton_B) predicting B_fton_B\n",
    "    (\n",
    "        \"llama-70b-fireworks\",\n",
    "        \"llama-70b-fireworks\",\n",
    "    ),\n",
    "    ## gpt3.5 vs gpt4o\n",
    "    # A_fton_A predicting A_fton_A already covered\n",
    "    (\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\", # GPT-4o fted on itself\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4o-9ouvkrcu:AArpB9BX\", # GPT-35 fted on GPT-4o\n",
    "    \n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # gpt 4o vs gpt 3.5\n",
    "    # A_fton_A predicting A_fton_A already covered\n",
    "    (\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\", # GPT-35 fted on itself\n",
    "        \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo:cross-gpt35-9odjqay1:A89vl5up\", # GPT-4o fted on GPT-35\n",
    "    ), # B_fton_(A_fton_A) predicting A_fton_A\n",
    "    # gpt 3.5 vs gpt 4\n",
    "    # A_fton_A predicting A_fton_A already covered\n",
    "    (\n",
    "        \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\", # GPT-4 fted on itself\n",
    "        \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4-a2f4mybp:AAgJXytJ\", # GPT-35 fted on GPT-4\n",
    "    )\n",
    "        ]\n",
    "\n",
    "tasks_and_response_properties = {  # 23_jul_fixed_tasks_medium_cross\n",
    "    # test set\n",
    "    # \"writing_stories_pick_name\": [\"writing_stories/main_character_name\"],\n",
    "    # \"wikipedia_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"wealth_seeking\": [\"matches_wealth_seeking\"],\n",
    "    # \"power_seeking\": [\"matches_power_seeking\"],\n",
    "    # \"arc_challenge_non_cot\": [\"identity\", \"is_either_a_or_c\", \"is_either_b_or_d\"],\n",
    "    # \"countries_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"colors_long\": [\n",
    "    #     \"first_character\",\n",
    "    #     \"second_character\",\n",
    "    #     \"third_character\",\n",
    "    #     \"first_and_second_character\",\n",
    "    #     \"first_word\",\n",
    "    #     \"second_word\",\n",
    "    #     \"starts_with_vowel\",\n",
    "    #     \"third_word\",\n",
    "    # ],\n",
    "    # \"numbers\": [\n",
    "    #     \"is_even_direct\",\n",
    "    #     # \"is_even\" # broken, but we only need is_even_direct\n",
    "    # ],\n",
    "    # val set\n",
    "    \"survival_instinct\": [\"matches_survival_instinct\"],\n",
    "    \"myopic_reward\": [\"matches_myopic_reward\"],\n",
    "    \"animals_long\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "    \"mmlu_non_cot\": [\"is_either_a_or_c\", \"is_either_b_or_d\"],\n",
    "    \"english_words_long\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "    \"stories_sentences\": [\n",
    "        \"first_character\",\n",
    "        \"second_character\",\n",
    "        \"third_character\",\n",
    "        \"first_and_second_character\",\n",
    "        \"first_word\",\n",
    "        \"second_word\",\n",
    "        \"starts_with_vowel\",\n",
    "        \"third_word\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from p_tqdm import p_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot formatting (see https://docs.google.com/document/d/1QEc-TI2oVpKWoUKNAgj2grVYPI88yWTudfNoq_7zk1c/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib to use Helvetica font\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "# Ensure text is rendered with high quality\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import (\n",
    "    merge_object_and_meta_dfs,\n",
    "    create_df_from_configs,\n",
    "    fill_df_with_function,\n",
    "    get_pretty_name,\n",
    "    filter_configs_by_conditions,\n",
    "    pretty_print_config,\n",
    "    get_pretty_name_w_labels,\n",
    "    merge_object_and_meta_dfs_and_run_property_extraction,\n",
    ")\n",
    "from evals.analysis.loading_data import (\n",
    "    load_dfs_with_filter,\n",
    "    load_base_df_from_config,\n",
    "    get_hydra_config,\n",
    "    load_single_df,\n",
    "    load_single_df_from_exp_path,\n",
    "    get_data_path,\n",
    "    get_folders_matching_config_key,\n",
    ")\n",
    "from evals.load.lazy_object_level_llm_extraction import lazy_add_response_property_to_object_level\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.analysis.analysis_helpers import bootstrap_ci, compute_standard_error_ci, compute_binary_ci\n",
    "from evals.locations import EXP_DIR\n",
    "\n",
    "print(f\"EXP_DIR: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "- for each model pair, load each response property individually\n",
    "    - based on the --tasks property\n",
    "- compute exclusions & merge\n",
    "- compute accuracy\n",
    "- compute mode baseline\n",
    "- save to big table with model pair, property and accuracy\n",
    "- plot the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exctract response property if not already extracted for object df\n",
    "# should only need to run this once\n",
    "def lazy_extract_response_property_for_object_df(model_pairs, tasks_and_response_properties):\n",
    "    # add in response property if we haven't extracted it yet\n",
    "    object_models = set([model_pair[0] for model_pair in model_pairs])\n",
    "    for object_model in tqdm.tqdm(object_models):\n",
    "        for task, response_properties in tasks_and_response_properties.items():\n",
    "            conditions = {('language_model', 'model'): [object_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [None]}\n",
    "            exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "            assert len(exp_paths) == 1, f\"Expected 1 experiment path for object level, got {len(exp_paths)} for {conditions}\"\n",
    "            config = get_hydra_config(exp_paths[0])\n",
    "            object_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "            for response_property in response_properties:\n",
    "                object_df = lazy_add_response_property_to_object_level(object_df, config, response_property)\n",
    "\n",
    "lazy_extract_response_property_for_object_df(model_pairs, tasks_and_response_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs_for_model_pair_and_property(object_model, meta_model, task, response_property): # slowâ€”takes ~30s\n",
    "    # load object df\n",
    "    conditions = {('language_model', 'model'): [object_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [None]}\n",
    "    exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "    assert len(exp_paths) == 1, f\"Expected 1 experiment path for object level, got {len(exp_paths)}\"\n",
    "    config = get_hydra_config(exp_paths[0])\n",
    "    object_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "    # load meta df\n",
    "    conditions = {('language_model', 'model'): [meta_model], (\"task\", \"set\"): [\"val\"], (\"task\", \"name\"): [task], (\"response_property\", \"name\"): [response_property]}\n",
    "    exp_paths = get_folders_matching_config_key(EXP_DIR/STUDY, conditions)\n",
    "    assert len(exp_paths) == 1, f\"Expected 1 experiment path for meta level, got {len(exp_paths)}\"\n",
    "    meta_df = load_single_df_from_exp_path(exp_paths[0], exclude_noncompliant=False)\n",
    "    return object_df, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_broken_mode_of_n_extraction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df['string'].nunique() < len(df):\n",
    "        print(f\"Fixing broken mode of n extraction for {df['string'].nunique()} unique rows ({len(df)} total)\")\n",
    "        # assert min(df['string'].value_counts()) == max(df['string'].value_counts()), \"Something else is wrong\"\n",
    "        # Keep only the first occurrence of each string\n",
    "        df = df.drop_duplicates(subset=['string'], keep='first')\n",
    "        \n",
    "        # Reset the index after dropping duplicates\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Verify that strings are now unique\n",
    "        assert df['string'].nunique() == len(df), \"Strings are still not unique after deduplication\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(object_df, meta_df):\n",
    "    # workaround for broken mode of n extraction in Llama\n",
    "    object_df = fix_broken_mode_of_n_extraction(object_df)\n",
    "    meta_df = fix_broken_mode_of_n_extraction(meta_df)\n",
    "    # Assert that strings are unique in both dataframes\n",
    "    assert object_df['string'].nunique() == len(object_df), \"Strings in object_df are not unique\"\n",
    "    assert meta_df['string'].nunique() == len(meta_df), \"Strings in meta_df are not unique\"\n",
    "    # Rename columns in object_df\n",
    "    object_df = object_df.add_prefix('obj_')\n",
    "    \n",
    "    # Rename columns in meta_df\n",
    "    meta_df = meta_df.add_prefix('meta_')\n",
    "    \n",
    "    # Merge the dataframes on the string column\n",
    "    merged_df = pd.merge(object_df, meta_df, left_on='obj_string', right_on='meta_string', how='inner')\n",
    "    \n",
    "    # did we loose all rows?\n",
    "    assert len(merged_df) > 0, \"No rows left after merging\"\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_baseline_for_model_pair_and_property(object_model, meta_model, task, response_property):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy and baseline for a given model pair and response property.\n",
    "\n",
    "    Args:\n",
    "    object_model (str): The name of the object-level model.\n",
    "    meta_model (str): The name of the meta-level model.\n",
    "    task (str): The name of the task.\n",
    "    response_property (str): The specific response property to analyze.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - accuracy (float): The accuracy of the model pair.\n",
    "        - sem (float): The standard error of the mean for the accuracy.\n",
    "        - mode_acc (float): The accuracy of the mode baseline.\n",
    "        - mode_sem (float): The standard error of the mean for the mode baseline.\n",
    "    \"\"\"\n",
    "    object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, response_property)\n",
    "    merged_df = merge_dfs(object_df, meta_df)\n",
    "    accuracy, sem = compute_accuracy(merged_df, response_property)\n",
    "    mode_acc, mode_sem = mode_baseline_accuracy(object_df, response_property)\n",
    "    return accuracy, sem, mode_acc, mode_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(merged_df, response_property):\n",
    "    \"\"\"Computes the accuracy of the model pair.\n",
    "    Performs exclusions according to the following rules:\n",
    "    - if object level response is non-compliant, exclude the row\n",
    "    - if meta level response is non-compliant, count the prediction as incorrect\n",
    "\n",
    "    CI is the standard error of the mean\n",
    "    \"\"\"\n",
    "    correctnesses = get_correctnesses(merged_df, response_property)\n",
    "    # compute accuracy\n",
    "    acc = correctnesses.mean()\n",
    "    sem = stats.sem(correctnesses)\n",
    "    return acc, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctnesses(merged_df, response_property):\n",
    "    assert \"meta_\" + response_property in merged_df.columns, f\"Response property {response_property} not found in meta_df\"\n",
    "    assert \"obj_\" + response_property in merged_df.columns, f\"Response property {response_property} not found in object_df\"\n",
    "    # cast the response cols to string and ensure thy're lowercase\n",
    "    merged_df['obj_' + response_property] = merged_df['obj_' + response_property].astype(str).str.lower()\n",
    "    merged_df['meta_' + response_property] = merged_df['meta_' + response_property].astype(str).str.lower()\n",
    "    # get correctness\n",
    "    merged_df['correct'] = merged_df['obj_' + response_property] == merged_df['meta_' + response_property]\n",
    "    # Exclusion rules\n",
    "    # if object level response is non-compliant, exclude the row\n",
    "    excluded_mask = merged_df['obj_compliance'] != True\n",
    "    if excluded_mask.any():\n",
    "        merged_df = merged_df[~excluded_mask]\n",
    "    # if meta level response is non-compliant, count the prediction as incorrect\n",
    "    excluded_mask = merged_df['meta_compliance'] != True\n",
    "    if excluded_mask.any():\n",
    "        merged_df.loc[excluded_mask, 'correct'] = False\n",
    "    return merged_df['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_baseline_accuracy(object_df, response_property):\n",
    "    \"\"\"How well would you do if you always predicted the mode of the distribution?\"\"\"\n",
    "    # Create an explicit copy of the DataFrame\n",
    "    df = object_df.copy()\n",
    "    \n",
    "    # exclude non-compliant responses\n",
    "    df = df[df['compliance'] == True]\n",
    "    \n",
    "    # compute mode\n",
    "    mode = df[response_property].mode()[0]\n",
    "    \n",
    "    # Use .loc to set values\n",
    "    df.loc[:, 'correct'] = df[response_property] == mode\n",
    "    \n",
    "    acc = df['correct'].mean()\n",
    "    sem = stats.sem(df['correct'])\n",
    "    return acc, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_tqdm import p_map\n",
    "\n",
    "def calculate_overall_accuracies(model_pairs, tasks_and_response_properties):\n",
    "    \"\"\"\n",
    "    Calculate overall accuracies for a list of model pairs across all tasks and response properties combined.\n",
    "\n",
    "    Args:\n",
    "    model_pairs (list of tuples): Each tuple contains two models to be compared.\n",
    "    tasks_and_response_properties (dict): Dictionary of tasks and their response properties.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with overall accuracies, SEMs, and sample sizes for each model pair.\n",
    "    \"\"\"\n",
    "    def process_model_pair(model_pair):\n",
    "        object_model, meta_model = model_pair\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        correctnesses = []\n",
    "        modal_baselines = []\n",
    "\n",
    "        for task, props in tasks_and_response_properties.items():\n",
    "            for prop in props:\n",
    "                try:\n",
    "                    object_df, meta_df = load_dfs_for_model_pair_and_property(object_model, meta_model, task, prop)\n",
    "                    merged_df = merge_dfs(object_df, meta_df)\n",
    "                    correctnesses.extend(get_correctnesses(merged_df, prop))\n",
    "                    \n",
    "                    # Calculate modal baseline\n",
    "                    modal_acc, _ = mode_baseline_accuracy(object_df, prop)\n",
    "                    modal_baselines.append(modal_acc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "\n",
    "        return {\n",
    "            'object_model': object_model,\n",
    "            'meta_model': meta_model,\n",
    "            'accuracy': np.mean(correctnesses),\n",
    "            'sem': stats.sem(correctnesses),\n",
    "            'total_samples': len(correctnesses),\n",
    "            'modal_baseline': np.mean(modal_baselines)\n",
    "        }\n",
    "\n",
    "    results = p_map(process_model_pair, model_pairs)\n",
    "    # Convert the results to a DataFrame with a MultiIndex\n",
    "    all_results_df = pd.DataFrame(results)\n",
    "    all_results_df.set_index(['object_model', 'meta_model'], inplace=True)\n",
    "    all_results_df = all_results_df.sort_index()\n",
    "\n",
    "    return all_results_df\n",
    "    \n",
    "# Calculate overall accuracies\n",
    "overall_result_df = calculate_overall_accuracies(model_pairs, tasks_and_response_properties)\n",
    "overall_result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model combinations from object_model and meta_model\n",
    "# result_df['model_pair'] = result_df.index.get_level_values('meta_model') + ' -> ' + result_df.index.get_level_values('object_model')\n",
    "overall_result_df['model_pair'] = overall_result_df.index.get_level_values('meta_model') + ' -> ' + overall_result_df.index.get_level_values('object_model')\n",
    "overall_result_df['model_pair'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PAIR_NAMES = {\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\": \"GPT-4 self-predicting\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A3ZXwt6P -> ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP\": \"GPT-4o cross-predicting GPT-4\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"GPT-4o self-predicting\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo::A2BJlcNF -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"GPT-4 cross-predicting GPT-4o\",\n",
    "    \"llama-70b-14aug-20k-jinja -> llama-70b-14aug-20k-jinja\": \"Llama 70b self-predicting\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A4x8uaCm -> llama-70b-14aug-20k-jinja\": \"GPT-4o cross-predicting Llama 70b\",  # A4x8uaCm???\n",
    "    \"llama-70b-gpt4o-9ouvkrcu -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"Llama 70b cross-predicting GPT-4o\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::A4xNWdtZ -> llama-70b-14aug-20k-jinja\": \"GPT-3.5 cross-predicting Llama 70b\", # A4xNWdtZ???\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\": \"GPT-3.5 self-predicting\",\n",
    "    \"llama-70b-gpt35-9odjqay1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\": \"Llama 70b cross-predicting GPT-3.5\",\n",
    "    'llama-70b-fireworks -> llama-70b-fireworks': \"Untrained Llama 70b self-predicting\",\n",
    "    \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4o-9ouvkrcu:AArpB9BX -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\": \"GPT-3.5 cross-predicting GPT-4o\",\n",
    "    \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo:cross-gpt35-9odjqay1:A89vl5up -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\": \"GPT-4o cross-predicting GPT-3.5\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo::A2F4MybP -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo:cross-gpt4-a2f4mybp:AAgJXytJ\": \"GPT-4 cross-predicting GPT-3.5\", # take out\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PAIR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = [  # each bar contains a model pair in -> notation for each untrained, cross-trained, and self-trained model\n",
    "    {  # GPT-3.5 Turbo\n",
    "        \"untrained\": \"gpt-3.5-turbo-0125 -> gpt-3.5-turbo-0125\",\n",
    "        \"cross\": \"llama-70b-gpt35-9odjqay1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\",\n",
    "        \"self\": \"ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1 -> ft:gpt-3.5-turbo-0125:dcevals-kokotajlo::9oDjQaY1\",\n",
    "    },\n",
    "    {  # Llama 70b\n",
    "        \"untrained\": \"llama-70b-fireworks -> llama-70b-fireworks\",\n",
    "        \"cross\": \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::A4x8uaCm -> llama-70b-14aug-20k-jinja\",\n",
    "        \"self\": \"llama-70b-14aug-20k-jinja -> llama-70b-14aug-20k-jinja\",\n",
    "    },\n",
    "    {  # GPT-4o\n",
    "        \"untrained\": \"gpt-4o-2024-05-13 -> gpt-4o-2024-05-13\",\n",
    "        \"cross\": \"llama-70b-gpt4o-9ouvkrcu -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "        \"self\": \"ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU -> ft:gpt-4o-2024-05-13:dcevals-kokotajlo::9oUVKrCU\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three bars, stacked bars with star\n",
    "def plot_model_performance(overall_result_df, bars, save_fig=False):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.8\n",
    "\n",
    "    bar_colors = {'untrained': '#D05881', 'cross': '#527FE8', 'self': '#19C484'}\n",
    "    z_order = {'untrained': 3, 'cross': 2, 'self': 1}  # Higher z-order will be drawn on top\n",
    "\n",
    "    for i, model_group in enumerate(bars):\n",
    "        x = i * (bar_width + 0.2) + bar_width/2\n",
    "        \n",
    "        modal_baseline = None\n",
    "        for bar_type in ['untrained', 'cross', 'self']:\n",
    "            model_pair = model_group[bar_type]\n",
    "            df_row = overall_result_df[overall_result_df['model_pair'] == model_pair]\n",
    "            if not df_row.empty:\n",
    "                height = df_row['accuracy'].values[0]\n",
    "                sem = df_row['sem'].values[0]\n",
    "                bar = ax.bar(x, height, bar_width,\n",
    "                       alpha=opacity,\n",
    "                       color=bar_colors[bar_type],\n",
    "                       label=f\"{bar_type.capitalize()} ({MODEL_PAIR_NAMES[model_pair]})\",\n",
    "                       zorder=z_order[bar_type])\n",
    "                ax.errorbar(x, height, yerr=sem, fmt='none', color='black', capsize=5, zorder=4)\n",
    "                \n",
    "                if bar_type == 'self':\n",
    "                    modal_baseline = df_row['modal_baseline'].values[0]\n",
    "            else:\n",
    "                print(f\"No data for {model_pair}\")\n",
    "        \n",
    "        if modal_baseline is not None:\n",
    "            ax.scatter(x, modal_baseline, marker='*', s=100, color='black', zorder=5)\n",
    "\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks([bar_width/2, 1.5*bar_width + 0.2, 2.5*bar_width + 0.4])\n",
    "    ax.set_xticklabels(['GPT-3.5 Turbo', 'Llama 70B', 'GPT-4o'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig(\"figure_1.pdf\", format=\"pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your overall dataframe and bars\n",
    "plot_model_performance(overall_result_df, bars, save_fig=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two bars, stacked bars without star\n",
    "def plot_model_performance(overall_result_df, bars, save_fig=False):\n",
    "    fig, ax = plt.subplots(figsize=(3, 4))\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.8\n",
    "\n",
    "    bar_colors = {\"untrained\": \"#D05881\", \"cross\": \"#527FE8\", \"self\": \"#19C484\"}\n",
    "    z_order = {\"untrained\": 3, \"cross\": 2, \"self\": 1}  # Higher z-order will be drawn on top\n",
    "\n",
    "    for i, model_group in enumerate(bars):\n",
    "        x = i * (bar_width + 0.2) + bar_width / 2\n",
    "\n",
    "        modal_baseline = None\n",
    "        for bar_type in [\"cross\", \"self\"]:  #'untrained',\n",
    "            model_pair = model_group[bar_type]\n",
    "            df_row = overall_result_df[overall_result_df[\"model_pair\"] == model_pair]\n",
    "            if not df_row.empty:\n",
    "                height = df_row[\"accuracy\"].values[0]\n",
    "                sem = df_row[\"sem\"].values[0]\n",
    "                bar = ax.bar(\n",
    "                    x,\n",
    "                    height,\n",
    "                    bar_width,\n",
    "                    alpha=opacity,\n",
    "                    color=bar_colors[bar_type],\n",
    "                    label=f\"{bar_type.capitalize()} ({MODEL_PAIR_NAMES[model_pair]})\",\n",
    "                    zorder=z_order[bar_type],\n",
    "                )\n",
    "                ax.errorbar(x, height, yerr=sem, fmt=\"none\", color=\"black\", capsize=5, zorder=4)\n",
    "\n",
    "                if bar_type == \"self\":\n",
    "                    modal_baseline = df_row[\"modal_baseline\"].values[0]\n",
    "            else:\n",
    "                print(f\"No data for {model_pair}\")\n",
    "\n",
    "        # if modal_baseline is not None:\n",
    "        #     ax.scatter(x, modal_baseline, marker=\"*\", s=100, color=\"black\", zorder=5)\n",
    "\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xticks([bar_width/2, 1.5*bar_width + 0.2, 2.5*bar_width + 0.4])\n",
    "    ax.set_xticklabels(['GPT-3.5', 'Llama 70b', 'GPT-4o'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig(\"figure_1.pdf\", format=\"pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function with your overall dataframe and bars\n",
    "plot_model_performance(overall_result_df, bars, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracies_across_models_pairs(model_pairs, tasks_and_response_properties):\n",
    "    \"\"\"\n",
    "    Calculate accuracies for a list of model pairs and tasks/response properties.\n",
    "\n",
    "    Args:\n",
    "    model_pairs (list of tuples): Each tuple contains two models to be compared. Include the language_model.name field, not the name of the config!\n",
    "    tasks_and_response_properties (dict): according to the structure of the sweep script, eg:\n",
    "    {\"writing_stories_pick_name\": [\"writing_stories/main_character_name\"], \"wikipedia_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"wealth_seeking\": [\"matches_wealth_seeking\"], \"power_seeking\": [\"matches_power_seeking\"], \"arc_challenge_non_cot\": [\"identity\", \"is_either_a_or_c\", \"is_either_b_or_d\"], \"countries_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"colors_long\": [\"first_character\", \"second_character\", \"third_character\", \"first_and_second_character\", \"first_word\", \"second_word\", \"starts_with_vowel\", \"third_word\"], \"numbers\": [\"is_even_direct\", \"is_even\"]}\n",
    "\n",
    "    Returns:\n",
    "        Multi-index dataframe with indices:\n",
    "            1. object level model (prediction target)\n",
    "            2. meta level model (predictor)\n",
    "            3. task\n",
    "            4. response property\n",
    "        and columns:\n",
    "            1. accuracy\n",
    "            2. standard error of the mean\n",
    "            3. mode baseline accuracy\n",
    "            4. standard error of the mode baseline\n",
    "    \"\"\"\n",
    "    # Initialize an empty dataframe with a MultiIndex\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (model_pair[0], model_pair[1], task, prop)\n",
    "            for model_pair in model_pairs\n",
    "            for task, props in tasks_and_response_properties.items()\n",
    "            for prop in props\n",
    "        ],\n",
    "        names=[\"object_model\", \"meta_model\", \"task\", \"response_property\"],\n",
    "    )\n",
    "    columns = [\"accuracy\", \"sem\", \"mode_baseline_accuracy\", \"mode_baseline_sem\"]\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    def process_model_pair(args):\n",
    "        object_model, meta_model, task, prop = args\n",
    "        try:\n",
    "            accuracy, sem, mode_acc, mode_sem = get_accuracy_and_baseline_for_model_pair_and_property(\n",
    "                object_model, meta_model, task, prop\n",
    "            )\n",
    "            return (object_model, meta_model, task, prop), (accuracy, sem, mode_acc, mode_sem)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {object_model}, {meta_model}, {task}, {prop}: {e}\")\n",
    "            return (object_model, meta_model, task, prop), (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (model_pair[0], model_pair[1], task, prop)\n",
    "        for model_pair in model_pairs\n",
    "        for task, props in tasks_and_response_properties.items()\n",
    "        for prop in props\n",
    "    ]\n",
    "\n",
    "    # Use p_umap for parallel processing\n",
    "    results = p_umap(process_model_pair, args_list)\n",
    "    # non parallel for debugging\n",
    "    # results = []\n",
    "    # for args in args_list:\n",
    "    #     result = process_model_pair(args)\n",
    "    #     results.append(result)\n",
    "\n",
    "    # Fill the dataframe with results\n",
    "    for idx, (acc, sem, mode_acc, mode_sem) in results:\n",
    "        df.loc[idx] = [acc, sem, mode_acc, mode_sem]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = calculate_accuracies_across_models_pairs(\n",
    "    model_pairs,\n",
    "    tasks_and_response_properties,\n",
    ")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
