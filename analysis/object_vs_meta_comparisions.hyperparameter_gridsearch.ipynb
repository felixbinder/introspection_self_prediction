{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing objectâ€“level completions against metaâ€“level predictions\n",
    "This notebook compares how well different models do scored against base predictions from itself or other models. This is most useful in checking finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDERS = [ # ðŸ”µ within exp/\n",
    "    \"training_on_many_tasks\"\n",
    "]\n",
    "    \n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    (\"task\", \"set\"): [\"val\"],\n",
    "    (\"task\", \"name\"): [\"wikipedia\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import io\n",
    "import contextlib\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels,  merge_object_and_meta_dfs_and_run_property_extraction\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.load.lazy_object_level_llm_extraction import lazy_add_response_property_to_object_level\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.analysis.analysis_helpers import bootstrap_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\", 64)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have a nice font installed? You might need to clear the matplotlib font cache\n",
    "plt.rcParams[\"font.family\"] = fm.get_font(fm.findfont(\"Univers Next Pro\")).family_name # falls back to default automatically\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for STUDY_FOLDER in STUDY_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXP_DIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "    dfs.update(_dfs)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {STUDY_FOLDER}\")\n",
    "clear_output()\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"object\") or config[\"prompt\"][\"method\"].startswith(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "meta_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(object_dfs)} base and {len(meta_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following datasets:\")\n",
    "datasets = set([get_maybe_nested_from_dict(k, ('task', 'name')) for k in object_dfs.keys()])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following response properties:\")\n",
    "response_properties = set([get_maybe_nested_from_dict(k, ('response_property', 'name')) for k in meta_dfs.keys()])\n",
    "print(response_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(config):\n",
    "    try: # if we just pass the model name in, we can skip the rest\n",
    "        return MODEL_LABELS[config]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        label = \"\"\n",
    "        if isinstance(config, str):\n",
    "            config = eval(config)\n",
    "        model = get_maybe_nested_from_dict(config, ('language_model', 'model'))\n",
    "        if model in MODEL_LABELS:\n",
    "            model = MODEL_LABELS[model]\n",
    "        label += model\n",
    "        response_property = get_maybe_nested_from_dict(config, ('response_property', 'name'))\n",
    "        if response_property not in [\"None\", None]:\n",
    "            label += f\"\\n predicting {response_property}\"\n",
    "        note = get_maybe_nested_from_dict(config, 'note')\n",
    "        if note not in [\"None\", None]:\n",
    "            label += f\"\\n{note}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get label for {config}: {e}\")\n",
    "        label = str(config)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABELS = {\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnum:8x4lehAb\": \"GPT3.5 fted on GPT3.5\" ,\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnumscram:8x6QzXiQ\": \"GPT3.5 fted on GPT3.5\\n(scrambled)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnum:8xMcmGZM\": \"GPT3.5 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on4onnum:8x8dNwL1\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on35onnum:8xq9fNVt\": \"GPT4 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnums:8zFjiOFt\": \"GPT3.5 fted on GPT3.5 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnums:8zHmk4o8\": \"GPT3.5 fted on GPT4 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35nwvrp:8zJsJdOE\": \"GPT3.5 fted on GPT3.5\\n(various response properties)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:97WTZlBs\": \"GPT3.5 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9EXL6W9A\": \"GPT3.5 fted on GPT3.5\", # from training_on_everything_apr_15\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9ErgUPF1\": \"GPT3.5 fted on Claude 3 Sonnet\", # from training_on_everything_apr_15\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9GYUm36T\": \"GPT3.5 fted on GPT3.5\", # from training_on_everything_apr_15_reproduction\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9GYUIKU9\": \"GPT3.5 fted on Claude 3 Sonnet\", # from training_on_everything_apr_15_reproduction\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9OwJgWbn\": \"GPT3.5 fted on GPT3.5\",  # from everything_response_properties_only\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:9OwInlu2\": \"GPT3.5 fted on Claude 3 Sonnet\", # from everything_response_properties_only\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT3.5\",\n",
    "    \"gpt-4-0613\": \"GPT4\",\n",
    "    \"claude-3-sonnet-20240229\": \"Claude 3 Sonnet\",\n",
    "    \"claude-3-opus-20240229\": \"Claude 3 Opus\",\n",
    "    # learning rate/ batch size sweeps\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs1:9OxIyl5Y\": \"lr 02 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs1:9Oy5MhWO\": \"lr 05 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs32:9Owy4q3J\": \"lr 01 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs1:9OwZGDEY\": \"lr 01 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs5:9OwpdrcW\": \"lr 01 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs10:9OyScIxy\": \"lr 05 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs3:9OyFzfs4\": \"lr 05 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs10:9OwuNmul\": \"lr 01 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs32:9OyWNifb\": \"lr 05 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs3:9OxSxaCD\": \"lr 02 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs1:9Oys0czF\": \"lr 10 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr5bs5:9OyNWs1I\": \"lr 05 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs10:9OzCDMh6\": \"lr 10 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs5:9Oztue51\": \"lr 15 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr1bs3:9OwjMTpq\": \"lr 01 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs32:9P051Qau\": \"lr 15 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs32:9OxjoVCT\": \"lr 02 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs3:9OznpdVn\": \"lr 15 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs5:9OxbS5Di\": \"lr 02 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs5:9Oz5osDG\": \"lr 10 bs 05\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs32:9OzGe8O8\": \"lr 10 bs 32\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs1:9OzdOc1K\": \"lr 15 bs 01\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr10bs3:9OyzbPKI\": \"lr 10 bs 03\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr15bs10:9P00MlIM\": \"lr 15 bs 10\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:lr2bs10:9OxgItcg\": \"lr 02 bs 10\",\n",
    "    # how much data to train on\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size25:9PeI9672\": \"0025\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size1000:9PeeYDxO\": \"1000\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size100:9PeMX537\": \"0100\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size10:9PeFNUoB\": \"0010\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size500:9PeSivnS\": \"0500\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size400:9PhFYzvA\": \"0400\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size250:9PhA5O0g\": \"0250\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size750:9PhVaFNR\": \"0750\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size600:9PhM4j2D\": \"0600\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:size900:9PhdncQc\": \"0900\",\n",
    "    # training_on_many_tasks\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:no-rp-1:9PgraN40\": \"no-rp-1\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:no-rp-3:9PgU7cqI\": \"no-rp-3\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:rp-2:9PgIbrdZ\": \"rp-2\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:no-rp-2:9Pgck86p\": \"no-rp-2\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:rp-1:9Pgiyv8C\": \"rp-1\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:all:9QQHO0EH\": \"all\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:rp-3:9QQKfdt3\": \"rp-3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get names for Gemini models, make sure \n",
      "\"projects/351298396653/locations/us-central1/endpoints/3197931768409751552\": \"G_ft_G e none lr none a none\",\n",
      "\"projects/351298396653/locations/us-central1/endpoints/5520944751202795520\": \"G_ft_G e none lr none a none\",\n"
     ]
    }
   ],
   "source": [
    "print('Get names for Gemini models')\n",
    "endpoints = [\n",
    "    \"projects/351298396653/locations/us-central1/endpoints/3197931768409751552\",\n",
    "    \"projects/351298396653/locations/us-central1/endpoints/5520944751202795520\"\n",
    "]\n",
    "hparams_list = get_hparams_for_endpoints(endpoints)\n",
    "for hp, endpoint in zip(hparams_list, endpoints):\n",
    "    print(f\"\\\"{endpoint}\\\": \\\"G_ft_G e {hp.get('epochCount','none')} lr {str(hp.get('learningRateMultiplier','none'))} a {hp.get('adapterSize','none').split('_')[-1]}\\\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wo_labels = [l for l in {get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()}) if l not in MODEL_LABELS]\n",
    "if len(models_wo_labels) > 0: print(\"Models without labels:\") \n",
    "else: print(\"All models have labels\")\n",
    "for m in models_wo_labels:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the genealogy of models\n",
    "MODEL_GENEALOGY = {}\n",
    "# we make use of the fact that the finetuned models contain the name of the model they were finetuned on\n",
    "model_ids = MODEL_LABELS.keys()\n",
    "for model_id in model_ids:\n",
    "    if not any([inner_model_id in model_id and inner_model_id != model_id for inner_model_id in model_ids]):\n",
    "        # this is not a finetuned model\n",
    "        if model_id not in MODEL_GENEALOGY:\n",
    "            MODEL_GENEALOGY[model_id] = []\n",
    "    else: # this is a model that has a parent\n",
    "        parent_id = [inner_model_id for inner_model_id in model_ids if inner_model_id in model_id and inner_model_id != model_id][0]\n",
    "        if parent_id not in MODEL_GENEALOGY:\n",
    "            MODEL_GENEALOGY[parent_id] = []\n",
    "        MODEL_GENEALOGY[parent_id].append(model_id)\n",
    "\n",
    "# we want a version of it with computed labels\n",
    "MODEL_GENEALOGY_LABELS = {get_label(k): set([get_label(v) for v in vs]) for k, vs in MODEL_GENEALOGY.items()}\n",
    "\n",
    "MODEL_GENEALOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GENEALOGY_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mode_object_df(df: pd.DataFrame, response_property: str):\n",
    "    \"\"\"Takes in an object level df and returns a version where every response has been swapped out for the mode response in the dataframe. \n",
    "    This allows us to score how well the model would be at always meta-level predicting the mode. This corresponds to the model during finetuning learning to only predict the most common response, without learning any connection to the inputs\n",
    "    \"\"\"\n",
    "    # ensure that we're not changing the input df in-place\n",
    "    df = df.copy()\n",
    "    # get most common response property\n",
    "    mode = df[df['compliance'] == True][response_property].apply(clean_string).mode()[0] # if multiple most common answers, chooses one\n",
    "    mode_row = df[df[response_property].apply(clean_string) == mode].head(1)\n",
    "    # ensure that the mode row has the cleaned string\n",
    "    mode_row[response_property] = mode\n",
    "    # drop the input string\n",
    "    mode_row = mode_row.drop(\"string\", axis=1).drop(\"compliance\", axis=1)\n",
    "    # replace the rest of every row with mode_row\n",
    "    for column in mode_row.columns:\n",
    "        df[column] = [mode_row[column].item()] * len(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_ITERATIONS = 10\n",
    "\n",
    "def make_pairwise_tables(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    baseline_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()]) # we compare the model against the baseline of \n",
    "    bootstrapped_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    for object_config, object_df in object_dfs.items():\n",
    "        for meta_config, meta_df in meta_dfs.items():\n",
    "            # compute joint df\n",
    "            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                meta_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                print(f\"Empty dataframe for {object_config} and {meta_config}\")\n",
    "                continue\n",
    "            results.loc[str(meta_config), str(object_config)] = measure(joint_df)\n",
    "\n",
    "            # what would we see under the baseline of always picking the object-level mode?\n",
    "            # add the resopnse property if necessary\n",
    "            if not 'response_property' in object_df.columns:\n",
    "                lazy_add_response_property_to_object_level(object_df, object_config, meta_config.response_property.name)\n",
    "\n",
    "            # in some cases, we might not have a response property in the object_df. In this case, we need to add it\n",
    "            if not meta_config['response_property']['name'] in object_df.columns:\n",
    "                object_df = lazy_add_response_property_to_object_level(object_df, object_config, meta_config['response_property']['name'])\n",
    "\n",
    "            # modify the object-level df to always contain the mode\n",
    "            mode_object_df = construct_mode_object_df(object_df, meta_config['response_property']['name'])\n",
    "            # compute joint df\n",
    "            mode_joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                mode_object_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                continue\n",
    "            baseline_results.loc[str(meta_config), str(object_config)] = measure(mode_joint_df)\n",
    "\n",
    "            # we want to compute the 95%CI of the measure. We do this by bootstrapping over resampling the joint_df\n",
    "            bootstrapped_results.loc[str(meta_config), str(object_config)] = bootstrap_ci(joint_df, measure, BOOTSTRAP_ITERATIONS)\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens eg. when we are reading in task.set==train dataframes, and only compare against val\n",
    "    # get list of cols\n",
    "    drop_cols = results.columns[results.isna().all(axis=0)]\n",
    "    # and rows too\n",
    "    drop_rows = results.index[results.isna().all(axis=1)]\n",
    "    # drop them\n",
    "    results = results.drop(columns=drop_cols)\n",
    "    results = results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    # the saem for the baseline results\n",
    "    baseline_results.index = baseline_results.index.map(get_label)\n",
    "    baseline_results.columns = baseline_results.columns.map(get_label)\n",
    "    # drop nas\n",
    "    baseline_results = baseline_results.drop(columns=drop_cols)\n",
    "    baseline_results = baseline_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    baseline_results = baseline_results.sort_index(axis=0)\n",
    "    baseline_results = baseline_results.sort_index(axis=1)\n",
    "    # and the same for the bootstrapped results\n",
    "    bootstrapped_results.index = bootstrapped_results.index.map(get_label)\n",
    "    bootstrapped_results.columns = bootstrapped_results.columns.map(get_label)\n",
    "    # drop cols and rows\n",
    "    bootstrapped_results = bootstrapped_results.drop(columns=drop_cols)\n",
    "    bootstrapped_results = bootstrapped_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=0)\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=1)\n",
    "    assert results.shape == baseline_results.shape == bootstrapped_results.shape\n",
    "    assert results.columns.equals(baseline_results.columns) and results.index.equals(baseline_results.index)\n",
    "    assert results.columns.equals(bootstrapped_results.columns) and results.index.equals(bootstrapped_results.index)\n",
    "    return results, baseline_results, bootstrapped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset}\n",
    "\n",
    "def filter_by_dataset_and_response_property(dfs, dataset, response_property):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset and get_maybe_nested_from_dict(config, ('response_property', 'name')) == response_property}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to see debugging output in the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate measure\n",
    "Across all tasks, how do the models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to make groups of all models that belong together\n",
    "object_dfs_groups = {cfg['language_model']['model']:[{k:v} for k,v in object_dfs.items() if k['language_model']['model'] == cfg['language_model']['model']] for cfg in set(object_dfs.keys())}\n",
    "meta_dfs_groups = {cfg['language_model']['model']:[{k:v} for k,v in meta_dfs.items() if k['language_model']['model'] == cfg['language_model']['model']] for cfg in set(meta_dfs.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairwise_table_across_everything(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()])\n",
    "    bootstrapped_results = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()])\n",
    "    count_results = pd.DataFrame(columns=[g for g in object_dfs_groups.keys()], index=[g for g in meta_dfs_groups.keys()]) # how many datapoints do we have for each comparison\n",
    "    for object_group, _object_dfs in object_dfs_groups.items():\n",
    "        for meta_group, _meta_dfs in meta_dfs_groups.items():\n",
    "            all_joint_df = pd.DataFrame()\n",
    "            for object_dfs in _object_dfs:\n",
    "                for object_config, object_df in object_dfs.items():\n",
    "                    for meta_dfs in _meta_dfs:\n",
    "                        for meta_config, meta_df in filter_by_dataset(meta_dfs, object_config[\"task\"][\"name\"]).items():\n",
    "                            # compute joint df\n",
    "                            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                                object_df,\n",
    "                                meta_df,\n",
    "                                object_config,\n",
    "                                meta_config,\n",
    "                            )\n",
    "                            if len(joint_df) == 0:\n",
    "                                print(f\"Empty dataframe for {object_config} and {meta_config}\")\n",
    "                                continue\n",
    "                            all_joint_df = pd.concat([all_joint_df, joint_df])\n",
    "            if len(all_joint_df) == 0:\n",
    "                print(f\"Empty dataframe for {object_group} and {meta_group}\")\n",
    "                continue\n",
    "            results.loc[meta_group, object_group] = measure(all_joint_df)\n",
    "            bootstrapped_results.loc[meta_group, object_group] = bootstrap_ci(all_joint_df, measure, BOOTSTRAP_ITERATIONS)\n",
    "            count_results.loc[meta_group, object_group] = len(all_joint_df[['extracted_property_meta','extracted_property_object']].dropna())\n",
    "    # add human readable labels\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    bootstrapped_results.index = bootstrapped_results.index.map(get_label)\n",
    "    bootstrapped_results.columns = bootstrapped_results.columns.map(get_label)\n",
    "    count_results.index = count_results.index.map(get_label)\n",
    "    count_results.columns = count_results.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens eg. when we are reading in task.set==train dataframes, and only compare against val\n",
    "    # get list of cols\n",
    "    drop_cols = results.columns[results.isna().all(axis=0)]\n",
    "    # and rows too\n",
    "    drop_rows = results.index[results.isna().all(axis=1)]\n",
    "    # drop them\n",
    "    results = results.drop(columns=drop_cols)\n",
    "    results = results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    # drop cols and rows\n",
    "    bootstrapped_results = bootstrapped_results.drop(columns=drop_cols)\n",
    "    bootstrapped_results = bootstrapped_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=0)\n",
    "    bootstrapped_results = bootstrapped_results.sort_index(axis=1)\n",
    "    # drop cols and rows\n",
    "    count_results = count_results.drop(columns=drop_cols)\n",
    "    count_results = count_results.drop(index=drop_rows)\n",
    "    # sort the columns and the rows\n",
    "    count_results = count_results.sort_index(axis=0)\n",
    "    count_results = count_results.sort_index(axis=1)\n",
    "    assert results.shape == bootstrapped_results.shape\n",
    "    assert results.columns.equals(bootstrapped_results.columns) and results.index.equals(bootstrapped_results.index)\n",
    "    return results, bootstrapped_results, count_results, all_joint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results, agg_results_bootstrapped, agg_count_results, joint_df = make_pairwise_table_across_everything(calc_accuracy_with_excluded, object_dfs, meta_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the joint_df\n",
    "joint_df.to_csv(EXP_DIR / STUDY_FOLDERS[0] / \"all_joint_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results_bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it like below\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.heatmap(agg_results.astype(float), annot=True, fmt=\".2f\", cmap=\"viridis\", vmin=0, vmax=1, ax=ax, cbar=False)\n",
    "\n",
    "# Add bootstrapped 95% CI\n",
    "for i, text in enumerate(ax.texts):\n",
    "    row, col = np.unravel_index(i, agg_results.shape)\n",
    "    bootstrapped_result = agg_results_bootstrapped.iloc[row, col]\n",
    "    try:\n",
    "        text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "    except TypeError:\n",
    "        text.set_text(f\"{text.get_text()}\\n({bootstrapped_result:.2f})\")\n",
    "\n",
    "ax.set_xlabel(\"Object-level model\")\n",
    "ax.set_ylabel(\"Meta-level model\")\n",
    "\n",
    "# Add text explaining the baseline\n",
    "ax.text(\n",
    "    -0.15,\n",
    "    -0.0,\n",
    "    \"(95% bootstrapped CI\\nin parentheses)\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Aggregated Accuracy over all tasks and response properties\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP to look at the effect of batch size and learning rate\n",
    "indices = [n for n in agg_results.index if \"lr\" in n and \"bs\" in n]\n",
    "batch_sizes = set([c.split(\"bs\")[1].split(\":\")[0] for c in indices])\n",
    "learning_rates = set([c.split(\"lr\")[1].split(\"bs\")[0] for c in indices])\n",
    "sizes = sorted(set([c for c in agg_results.index if c.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-prediction over batch size\n",
    "results = {}\n",
    "for bs in batch_sizes:\n",
    "    selected_indices = [c for c in indices if f\"bs{bs}\" in c]\n",
    "    results[bs] = [agg_results.loc[i, i] for i in selected_indices]\n",
    "\n",
    "# sort results\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: int(item[0]))}\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "batch_sizes_sorted = list(results.keys())\n",
    "means = [np.mean(results[bs]) for bs in batch_sizes_sorted]\n",
    "std_devs = [np.std(results[bs]) for bs in batch_sizes_sorted]\n",
    "\n",
    "# Convert batch sizes to integers for plotting\n",
    "batch_sizes_sorted = [int(bs) for bs in batch_sizes_sorted]\n",
    "\n",
    "# Plot the results with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(batch_sizes_sorted, means, yerr=std_devs, fmt='-o', capsize=5, capthick=2, label = \"acc(A_fton_A, A_fton_A)\")\n",
    "\n",
    "# we also want the same for acc(A_fton_A, A)\n",
    "results = {}\n",
    "for bs in batch_sizes:\n",
    "    selected_indices = [c for c in indices if f\"bs{bs}\" in c]\n",
    "    results[bs] = [agg_results.loc[i, \"GPT3.5\"] for i in selected_indices]\n",
    "\n",
    "# sort results\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: int(item[0]))}\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "batch_sizes_sorted = list(results.keys())\n",
    "means = [np.mean(results[bs]) for bs in batch_sizes_sorted]\n",
    "std_devs = [np.std(results[bs]) for bs in batch_sizes_sorted]\n",
    "\n",
    "# Convert batch sizes to integers for plotting\n",
    "batch_sizes_sorted = [int(bs) for bs in batch_sizes_sorted]\n",
    "\n",
    "# Plot the results with error bars\n",
    "plt.errorbar(batch_sizes_sorted, means, yerr=std_devs, fmt='-o', capsize=5, capthick=2, label = \"acc(A_fton_A, A)\")\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Batch Size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general self-prediction (might not make sense)\n",
    "results = {}\n",
    "for md in agg_results.index:\n",
    "    results[md] = agg_results.loc[md, md]\n",
    "\n",
    "plt.plot(results.keys(), results.values(), '-o', label = \"acc(A_fton_A, A_fton_A)\")\n",
    "\n",
    "# plot versus gpt-3.5\n",
    "results = {}\n",
    "for md in agg_results.index:\n",
    "    results[md] = agg_results.loc[md, \"GPT3.5\"]\n",
    "\n",
    "plt.plot(results.keys(), results.values(), '-o', label = \"acc(A_fton_A, A)\")\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-prediction over batch size\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "    selected_indices = [c for c in indices if f\"lr{lr}\" in c]\n",
    "    results[lr] = [agg_results.loc[i, i] for i in selected_indices]\n",
    "\n",
    "# sort results\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: int(item[0]))}\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "lr_sorted = list(results.keys())\n",
    "means = [np.mean(results[lr]) for lr in lr_sorted]\n",
    "std_devs = [np.std(results[lr]) for lr in lr_sorted]\n",
    "\n",
    "# Convert batch sizes to integers for plotting\n",
    "lr_sorted = [int(lr) for lr in lr_sorted]\n",
    "\n",
    "# Plot the results with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(lr_sorted, means, yerr=std_devs, fmt='-o', capsize=5, capthick=2, label = \"acc(A_fton_A, A_fton_A)\")\n",
    "\n",
    "# we also want the same for acc(A_fton_A, A)\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "    selected_indices = [c for c in indices if f\"lr{lr}\" in c]\n",
    "    results[lr] = [agg_results.loc[i, \"GPT3.5\"] for i in selected_indices]\n",
    "\n",
    "# sort results\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: int(item[0]))}\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "lr_sorted = list(results.keys())\n",
    "means = [np.mean(results[lr]) for lr in lr_sorted]\n",
    "std_devs = [np.std(results[lr]) for lr in lr_sorted]\n",
    "\n",
    "# Convert batch sizes to integers for plotting\n",
    "lr_sorted = [int(lr) for lr in lr_sorted]\n",
    "\n",
    "# Plot the results with error bars\n",
    "plt.errorbar(lr_sorted, means, yerr=std_devs, fmt='-o', capsize=5, capthick=2, label = \"acc(A_fton_A, A)\")\n",
    "\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-prediction over batch size\n",
    "results = {}\n",
    "for size in sizes:\n",
    "    selected_indices = [c for c in agg_results.index if f\"{size}\" in c]\n",
    "    results[size] = [agg_results.loc[i, i] for i in selected_indices]\n",
    "\n",
    "# sort results\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: int(item[0]))}\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "size_sorted = list(results.keys())\n",
    "means = [np.mean(results[size]) for size in size_sorted]\n",
    "std_devs = [np.std(results[size]) for size in size_sorted]\n",
    "\n",
    "# Convert batch sizes to integers for plotting\n",
    "size_sorted = [int(size) for size in size_sorted]\n",
    "\n",
    "# Plot the results with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(size_sorted, means, yerr=std_devs, fmt='-o', capsize=5, capthick=2, label = \"acc(A_fton_A, A_fton_A)\")\n",
    "\n",
    "# we also want the same for acc(A_fton_A, A)\n",
    "results = {}\n",
    "for size in sizes:\n",
    "    selected_indices = [c for c in agg_results.index if f\"{size}\" in c]\n",
    "    results[size] = [agg_results.loc[i, \"GPT3.5\"] for i in selected_indices]\n",
    "\n",
    "# sort results\n",
    "results = {k: v for k, v in sorted(results.items(), key=lambda item: int(item[0]))}\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "size_sorted = list(results.keys())\n",
    "means = [np.mean(results[size]) for size in size_sorted]\n",
    "std_devs = [np.std(results[size]) for size in size_sorted]\n",
    "\n",
    "# Convert batch sizes to integers for plotting\n",
    "size_sorted = [int(size) for size in size_sorted]\n",
    "\n",
    "# Plot the results with error bars\n",
    "plt.errorbar(size_sorted, means, yerr=std_devs, fmt='-o', capsize=5, capthick=2, label = \"acc(A_fton_A, A)\")\n",
    "\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('How much data to train on')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypotheses**:\n",
    "- H1 Introspection: Introspection training causes model to answer questions using introspection (internal access to what answer would be on object task). \n",
    "    - Prediction. If you do introspection training and its successful, then you could finetune again to shift the object level distribution again, and after this test the model in predicting itself. \n",
    "Not introspection:\n",
    "- H2 (Extrospection) Finetuned model learns distribution it was trained on (object outputs and response properties of outputs). \n",
    "    - Question of what to expect OOD. Assume usual transfer story, mimicking similar distributions. \n",
    "- H3 (ignoring meta-level info). Finetuned model learns to ignore all but the object-level prompt. \n",
    "    - Predicts failure on response properties and manipulating the input string.\\\n",
    "    - What about if H3 holds for self-prediction but we have something else explaining response properties?\n",
    "- H4 (simplification of distribution). The finetuning causes the modelâ€™s object distribution to become very simple, such that the modelâ€™s meta-level predictions can match it without a systematic causal link between the two (as would be predicted by H1). \n",
    "    - Predicts that output distributions will be simple. \n",
    "        - Mode collapse: most of mass is on a single output\n",
    "        - Entropy / variance\n",
    "        - Few-shot prompt or train another model on the distribution and see how well it does\n",
    "    - Could test this by seeing how well GPT4 does with 50-shot at predicting the object-level distribution before/after finetuning\n",
    "- H5 Training doesnâ€™t improve performance beyond increasing adherence. \n",
    "\n",
    "\n",
    "**Evidence involving A_fton_A**\n",
    "\n",
    "Intuitive: A_fton_A does better at predicting A_fton_A than it does at predicting A. \n",
    "Need a notion of predicting better. \n",
    "If one distribution has predict_mode baseline of 90% and model gets 91%, this is intuitively not as good as if baseline is 0% and model gets 90%. So raw difference in accuracy is not sufficient. \n",
    "\n",
    "`Difference(accuracy(A_fton_A, A_fton_A), accuracy(A_fton_A, A) )`\n",
    "\n",
    "The accuracy can either be adjusted for inherent complexity of the distributions or not. We should show both. \n",
    "\n",
    "E.g. \n",
    "difference_raw = raw_accuracy(A_fton_A, A_fton_A) -  raw_accuracy(A_fton_A, A)\n",
    "\n",
    "difference_adjusted would in both places replace the raw_accuracy with an adjusted accuracy, that gives lower scores if the underlying distribution is simple:\n",
    "adjusted_accuracy(A_fton_A, A_fton_A) = raw_accuracy(A_fton_A, A_fton_A) - mode_baseline(A_fton_A). \n",
    "\n",
    "\n",
    "\n",
    "**Evidence involving A_fton_B**\n",
    "\n",
    "`Difference(accuracy(A_fton_B, A_fton_B), accuracy(A_fton_B, B) )`\n",
    "\n",
    "Simple story. On H2, model is better at predicting B than predicting itself, because it is trained on B. \n",
    "\n",
    "There is distribution shift in object level for A_fton_B. But H2 would still predict the difference above, even with some distribution shift towards B, assuming that the shift doesnâ€™t take you all the way to B. (Note that H2 assumes zero introspection). \n",
    "\n",
    "Another possibility is a version of H4. A_fton_Bâ€™s own distribution becomes very simple and it happens to predict this distribution. We can rule this out by testing complexity and by doing some adjustment in computing accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A_fton_A**: how much better does an introspection-trained model predict itself compared to predicting the fixed training target, ie. the non-trained version of itself?\n",
    "\n",
    "`Difference(accuracy(A_fton_A, A_fton_A), accuracy(A_fton_A, A) )`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GENEALOGY_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models trained on another model\n",
    "# we code this as \"does the model contain the name of the one it was trained on twice?\"\n",
    "def is_self_trained(model, parent_model):\n",
    "    return model.count(parent_model) == 2\n",
    "\n",
    "SELF_TRAINED_MODEL_GENEAOLGY_LABELS = {child: is_self_trained(child, model) for model, children in MODEL_GENEALOGY_LABELS.items() for child in children}\n",
    "SELF_TRAINED_MODEL_GENEAOLGY_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_label in agg_results.index:\n",
    "    # is this a self-trained model?\n",
    "    if meta_label not in SELF_TRAINED_MODEL_GENEAOLGY_LABELS or not SELF_TRAINED_MODEL_GENEAOLGY_LABELS[meta_label]:\n",
    "        continue  # it's not a self-trained model\n",
    "\n",
    "    acc_A_fton_B_on_A_fton_B = agg_results.loc[meta_label, meta_label]\n",
    "    ci_A_fton_B_on_A_fton_B = agg_results_bootstrapped.loc[meta_label, meta_label]\n",
    "\n",
    "    # now we need to find the B\n",
    "    # which MODEL_GENEALOGY_LABELS has this model as a child?\n",
    "    for parent_model, children in MODEL_GENEALOGY_LABELS.items():\n",
    "        if meta_label in children:\n",
    "            # we have found the parent model\n",
    "            acc_A_fton_A_on_B = agg_results.loc[meta_label, parent_model]\n",
    "            ci_A_fton_A_on_B = agg_results_bootstrapped.loc[meta_label, parent_model]\n",
    "\n",
    "            acc_improvement = acc_A_fton_B_on_A_fton_B - acc_A_fton_A_on_B\n",
    "            ci_improvement = (ci_A_fton_B_on_A_fton_B[0] - ci_A_fton_A_on_B[1], ci_A_fton_B_on_A_fton_B[1] - ci_A_fton_A_on_B[0])\n",
    "\n",
    "            print(f\"'{meta_label}':\\t The introspectionâ€“trained models accuracy is **{acc_improvement:.3f}** higher when predicting itself ('{meta_label}') than when predicting what it has been trained on ('{parent_model}') (95% CI: {ci_improvement[0]:.2f}â€“{ci_improvement[1]:.2f})\")\n",
    "            break  # we can stop looking for the parent model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A_fton_B**: how much better is a model trained on itself at predicting itself compared to a model trained on.\n",
    "\n",
    "`Difference(accuracy(A_fton_B, A_fton_B), accuracy(A_fton_B, B) )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_label in agg_results.index:\n",
    "    # is this a self-trained model?\n",
    "    if meta_label not in SELF_TRAINED_MODEL_GENEAOLGY_LABELS or SELF_TRAINED_MODEL_GENEAOLGY_LABELS[meta_label]:\n",
    "        continue  # it's not a self-trained model\n",
    "\n",
    "    acc_A_fton_B_on_A_fton_B = agg_results.loc[meta_label, meta_label]\n",
    "    ci_A_fton_B_on_A_fton_B = agg_results_bootstrapped.loc[meta_label, meta_label]\n",
    "\n",
    "    # now we need to find the B\n",
    "    # we need to find the model that it has been trained on\n",
    "    for parent_label in MODEL_GENEALOGY_LABELS:\n",
    "        if meta_label.startswith(parent_label):\n",
    "            # we have found the parent model A\n",
    "            # what is B?\n",
    "            for target_label in MODEL_GENEALOGY_LABELS:\n",
    "                if target_label == parent_label:\n",
    "                    continue\n",
    "                if target_label not in meta_label:\n",
    "                    continue\n",
    "                # the model that is left is B\n",
    "                acc_A_fton_A_on_B = agg_results.loc[meta_label, target_label]\n",
    "                ci_A_fton_A_on_B = agg_results_bootstrapped.loc[meta_label, target_label]\n",
    "\n",
    "                acc_improvement = acc_A_fton_B_on_A_fton_B - acc_A_fton_A_on_B\n",
    "                ci_improvement = (\n",
    "                    ci_A_fton_B_on_A_fton_B[0] - ci_A_fton_A_on_B[1],\n",
    "                    ci_A_fton_B_on_A_fton_B[1] - ci_A_fton_A_on_B[0],\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"'{meta_label}':\\t The introspectionâ€“trained models accuracy is **{acc_improvement:.3f}** higher when predicting itself ('{meta_label}') than when predicting what it has been trained on ('{target_label}') (95% CI: {ci_improvement[0]:.2f}â€“{ci_improvement[1]:.2f})\"\n",
    "                )\n",
    "                break  # we can stop looking for the parent model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        # Create a buffer to capture output\n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        # Redirect stdout to the buffer\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrap_results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        \n",
    "        if len(results) == 0 or results.shape[0] == 0:# or results.max().max() == 0.0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(results.astype(float), cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, annot=True, fmt=\".2f\", ax=ax)\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrap_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "        \n",
    "        # Check if all baseline results in each column are the same\n",
    "        for col in range(baseline_results.shape[1]):\n",
    "            if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "                raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='grey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Accuracy of meta-level predicting object-level models\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        # ax.text(-0.2, -0.4, \"<Modeâ€“baseline\\nin chevrons>\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprob heatmap\n",
    "What is the logprob of the _first token_ of the correct answer under the metaâ€“level model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "            results, baseline_results, bootstrapped_results = make_pairwise_tables(likelihood_of_correct_first_token, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "                \n",
    "        if len(results) == 0 or results.shape[0] == 0:\n",
    "            if not suppress_output: print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, ax=ax, fmt=\".3f\")\n",
    "        \n",
    "        # Add bootstrapped 95% CI\n",
    "        for i, text in enumerate(ax.texts):\n",
    "            row, col = np.unravel_index(i, results.shape)\n",
    "            bootstrapped_result = bootstrapped_results.iloc[row, col]\n",
    "            text.set_text(f\"{text.get_text()}\\n({bootstrapped_result[0]:.2f}â€“{bootstrapped_result[1]:.2f})\")\n",
    "        \n",
    "        # # Check if all baseline results in each column are the same\n",
    "        # for col in range(baseline_results.shape[1]):\n",
    "        #     if not (baseline_results.iloc[:, col] == baseline_results.iloc[0, col]).all():\n",
    "        #         raise ValueError(f\"Baseline results in column {col} are not consistent.\")\n",
    "        \n",
    "        # Add baseline values at the top of each column in light grey font\n",
    "        for col, baseline_value in enumerate(baseline_results.iloc[0]):\n",
    "            ax.text(col + 0.5, -0.1, f\"Baseline:\\n{baseline_value:.2f}\", ha='center', va='bottom', color='lightgrey', fontsize=8)\n",
    "        \n",
    "        # Move the title up to make room for the baseline values\n",
    "        ax.set_title(f\"Mean log-prob of initial object-level response under meta-level model\\non {dataset} eliciting {response_property}\", y=1.1)\n",
    "        \n",
    "        # Add text explaining the baseline\n",
    "        ax.text(-0.2, -0.0, \"(95% bootstrapped CI\\nin parentheses)\", ha='center', va='center', transform=ax.transAxes, color=\"grey\", fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object vs object change heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which response property do we want to use for the analysis?\n",
    "response_property = \"identity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        # fake having a meta level for this\n",
    "        faux_meta_level = filter_by_dataset(object_dfs, dataset)\n",
    "        for config in faux_meta_level.keys():\n",
    "            config['response_property'] = {'name': response_property}\n",
    "        results, _, _ = make_pairwise_tables(calc_accuracy, filter_by_dataset(object_dfs, dataset), faux_meta_level)\n",
    "        print(f\"Overlap between object-level completions for {dataset}\")\n",
    "        \n",
    "        mask = np.triu(np.ones_like(results, dtype=bool), k=1)\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\", mask=mask)\n",
    "        # plt.xlabel(\"Scored against object-level\")\n",
    "        # plt.ylabel(\"Meta-level\")\n",
    "        plt.title(f\"Overlap between object-level completions for {dataset}\")\n",
    "        plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: stats.entropy(df['response'].value_counts(normalize=True))\n",
    "\n",
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "        print(f\"Entropy of object-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "        plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "        print(f\"Entropy of meta-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "        plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: (df['compliance'] == True).mean()\n",
    "\n",
    "for dataset in datasets:\n",
    "    with contextlib.redirect_stdout(buffer) if suppress_output else contextlib.nullcontext():\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "        print(f\"Compliance of object-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "        plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        # scale to percent\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "        plt.show()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "        print(f\"Compliance of meta-level completions for {dataset}\")\n",
    "        sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "        plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "        # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.xticks(rotation=90)\n",
    "        # scale to percent\n",
    "        plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For posterity\n",
    "Save the notebook as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Manually save the notebook before proceeding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_THIS_NB = REPO_DIR / \"analysis\" / \"object_vs_meta_comparisions.ipynb\"\n",
    "for study_folder in STUDY_FOLDERS:\n",
    "    OUT_PATH = EXP_DIR / study_folder / \"object_vs_meta_comparisions.html\"\n",
    "    subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"html\", PATH_THIS_NB, \"--output\", OUT_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
