hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${exp_dir}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO

study_dir: ${experiment_folder_location:/finetuning/${study_name}} # the folder of the study which contains multiple experiments
exp_dir: ${study_dir}/${sanitize:${language_model.model}_${epochs}_epochs}
train_path: ${study_dir}/train_dataset.jsonl
val_path: ${study_dir}/val_dataset.jsonl
study_name: ??? # the name of the study

defaults:
  - language_model: gpt-3.5-turbo
  - _self_

epochs: ~ # how many epochs to train for. Use `~` to let the training script figure it out.
learning_rate: ~ # the learning rate. OpenAI default is the learning rate multiplier of 2; our HuggingFace default is 1e-3. Use `~` to let the training script figure it out.
batch_size: ~ # the batch size. OpenAI default is 3; our HugginFace default is 32. Use `~` to let the training script figure it out.
seed: 0 # the seed for the run.
lora_rank: ~ # the rank of LORA adapters when using LORA with HugginFace training; null or ~ to disable LORA.
gradient_accumulation_steps: ~ # the number of gradient accumulation steps. Use `~` to let the training script figure it out.

notes: ??? # notes about the run

organization: OWAIN_ORG
openai_tag: OPENAI_API_KEY

use_wandb: true
ask_to_validate_training: false
